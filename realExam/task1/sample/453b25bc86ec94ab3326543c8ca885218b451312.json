{"paper_id": "453b25bc86ec94ab3326543c8ca885218b451312", "metadata": {"title": "Efficient Noise-Blind \u2113 1 -Regression of Nonnegative Compressible Signals", "authors": [{"first": "Hendrik", "middle": ["Bernd"], "last": "Petersen", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Bubacarr", "middle": [], "last": "Bah", "suffix": "", "affiliation": {}, "email": ""}, {"first": "Peter", "middle": [], "last": "Jung", "suffix": "", "affiliation": {}, "email": ""}]}, "abstract": [{"text": "In compressed sensing the goal is to recover a signal from as few as possible noisy, linear measurements. The general assumption is that the signal has only a few non-zero entries. Given an estimate for the noise level a common convex approach to recover the signal is basis pursuit denoising (BPDN). If the measurement matrix has the robust null space property with respect to the \u21132-norm, BPDN obeys stable and robust recovery guarantees. In the case of unknown noise levels, nonnegative least squares recovers non-negative signals if the measurement matrix fulfills an additional property (sometimes called the M +criterion). However, if the measurement matrix is the biadjacency matrix of a random left regular bipartite graph it obeys with a high probability the null space property with respect to the \u21131-norm with optimal parameters. Therefore, we discuss non-negative least absolute deviation (NNLAD). For these measurement matrices, we prove a uniform, stable and robust recovery guarantee. Such guarantees are important, since binary expander matrices are sparse and thus allow for fast sketching and recovery. We will further present a method to solve the NNLAD numerically and show that this is comparable to state of the art methods. Lastly, we explain how the NNLAD can be used for group testing in the recent COVID-19 crisis and why contamination of specimens may be modeled as peaky noise, which favors \u21131 based data fidelity terms.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Since it has been realized that many signals admit a sparse representation in some frames, the question arose whether or not such signals can be recovered from less samples than the dimension of the domain by abusing the low dimensional structure of the signal. The question was already answered positively in the beginning of the millennium [CRT06] [Don06] . By now there are multiple different decoders to recover a sparse signal from noisy measurements with robust recovery guarantees. Most of them however rely on some form of tuning, depending on either the signal or the noise. The basis pursuit denoising (BPDN) requires an upper bound on the norm of the noise [FR13, Theorem 4 .22], the least shrinkage and selection operator (LASSO) an estimate on the \u2113 1 -norm of the signal [HTW15, Theorem 11.1], the iterative hard thresholding an upper bound for the sparsity of the signal [FR13, Theorem 6.21], and the Lagrangian version of LASSO allegedly needs to be tuned in the order of the the noise level [HTW15, Theorem 11.1 ]. If these side information is not known a priori many decoders yield either no recovery guarantees or, in their imperfect tuned versions, yield sub-optimal estimation errors [FR13, Theorem 11.12 ]. Even though the problem of sparse recovery from under-sampled measurements has been answered long ago, finding tuning free decoders that achieve robust recovery guarantees is still a topic of interest. The most prominent achievement for that is the non-negative least squares (NNLS) [BEZ08] [DT10] [WXT11] [SH11] [SH13] . It is completely tuning free [KKRT16] and in [KJ18] [ SJC19] it was proven that it achieves robust recovery guarantees if the measurement matrix consists of certain independent sub-Gaussian random variables. We will replace the least squares with an arbitrary norm and obtain the non-negative least residual (NNLR). We prove recovery guarantees under similar conditions as the NNLS. In particular we consider the case where we minimize the \u2113 1 -norm of the residual (NNLAD) and give a recovery guarantee if the measurement matrix is a random walk matrix of a uniformly at random drawn D-left regular bipartite graph. This has two crucial advantages over the NNLS. The NNLS with sub-Gaussian measurement matrix might lose some level of sparsity for the recovery guarantee, while the left regularity implies that the sparsity level is being preserved when decoding with NNLAD and measuring with a random walk matrices of a left regular graph. On the other hand the need for fast encoding and decoding favors sparse measurement matrices over dense measurement matrices. The encoding time can be reduced to a small fraction by using sparse matrix multiplications for encoding. However, this also affects the decoding time, since decoders are often motivated as first order methods of an optimization problem minimizing the residual, so that in every iteration an iterate is being encoded. Thus, sparse measurement matrices often also allow for fast decoding. Here the NNLAD has crucial advantages. Using [CP11] we can solve the NNLAD with a first order method of a single optimization problem with a sparse measurement matrix. Other state of the art decoders often use non-convex optimization, computational complex projections or need to solve multiple different optimization problems. For instance, to solve the BPDN given a tuning parameter a common approach is to solve a sequence of LASSO problems to approximate where the Pareto curve attains the value of the tuning parameter of BPDN [vdBF09] . Note that other approaches for non-negative recovery are possible. In [DT05] they used a non-negative basis pursuit. For super resolution a problem really closely related to the NNLAD has been studied in [MC16] .", "cite_spans": [{"start": 342, "end": 349, "text": "[CRT06]", "ref_id": "BIBREF6"}, {"start": 350, "end": 357, "text": "[Don06]", "ref_id": "BIBREF7"}, {"start": 668, "end": 674, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 675, "end": 684, "text": "Theorem 4", "ref_id": null}, {"start": 1008, "end": 1015, "text": "[HTW15,", "ref_id": "BIBREF13"}, {"start": 1016, "end": 1028, "text": "Theorem 11.1", "ref_id": null}, {"start": 1205, "end": 1211, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 1212, "end": 1225, "text": "Theorem 11.12", "ref_id": null}, {"start": 1512, "end": 1519, "text": "[BEZ08]", "ref_id": "BIBREF2"}, {"start": 1527, "end": 1534, "text": "[WXT11]", "ref_id": "BIBREF30"}, {"start": 1542, "end": 1548, "text": "[SH13]", "ref_id": "BIBREF25"}, {"start": 1580, "end": 1588, "text": "[KKRT16]", "ref_id": "BIBREF17"}, {"start": 1596, "end": 1602, "text": "[KJ18]", "ref_id": "BIBREF16"}, {"start": 1605, "end": 1611, "text": "SJC19]", "ref_id": "BIBREF26"}, {"start": 3051, "end": 3057, "text": "[CP11]", "ref_id": "BIBREF5"}, {"start": 3538, "end": 3546, "text": "[vdBF09]", "ref_id": "BIBREF28"}, {"start": 3619, "end": 3625, "text": "[DT05]", "ref_id": "BIBREF9"}, {"start": 3753, "end": 3759, "text": "[MC16]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Introduction"}, {"text": "For K \u2208 N we denote the set of integers from 1 to K by [K] . For a set T \u2282 [N ] we denote the number of elements in T by # (T ). Vectors are denoted by lower case bold face symbols, while its corresponding components are denoted by lower case italic letters. Matrices are denoted by upper case bold face symbols, while its corresponding components are denoted by upper case italic letters. For x \u2208 R N we denote its \u2113 pnorms by x p . Given A \u2208 R M\u00d7N we denote the its operator norm as operator from \u2113 q to \u2113 p by A q\u2192p := sup v\u2208R N , v q \u22641 Av p . By R N + we denote the non-negative orthant. Given a closed convex set C \u2282 R N , we denote the projection onto C, i.e. the unique minimizer of argmin z\u2208C 1 2 z \u2212 v 2 2 , by P C (v). For a vector x \u2208 R N and a set T \u2282 [N ], x| T denotes the vector in R N , whose n-th component is x n if n \u2208 T and 0 else. Given N, S \u2208 N we will often need sets T \u2282 [N ] with # (T ) \u2264 S and we abbreviate this by # (T ) \u2264 S if no confusing is possible. Given a measurement matrix A \u2208 R M\u00d7N a decoder is any map Q A : R M \u2192 R N . A signal is any possible x \u2208 R N . If x \u2208 R N + = z \u2208 R N : z n \u2265 0 for all n \u2208 [N ] , we say the signal is non-negative and write shortly x \u2265 0. If additionally x n > 0 for all n \u2208 [N ], we write x > 0. An observation is any possible input of a decoder, i.e. all y \u2208 R M . We allow all possible inputs of the decoder as observation, since in general the transmitted codeword Ax is disturbed by some noise. Thus, given a signal x and an observation y we call e := y \u2212 Ax the noise. A signal x is called S-sparse if x 0 := # ({n \u2208 [N ] : x n = 0}) \u2264 S. We denote the set of S-sparse vectors by", "cite_spans": [{"start": 55, "end": 58, "text": "[K]", "ref_id": null}], "ref_spans": [], "section": "Preliminaries"}, {"text": "Given some S \u2208 [N ] the compressibility of a signal x can be measured by d 1 (x, \u03a3 S ) := inf z\u2208\u03a3S x \u2212 z 1 . Given N and S, the general non-negative compressed sensing task is to find a measurement matrix A \u2208 R M\u00d7N and a decoder Q A : R M \u2192 R N with M as small as possible such that the following holds true: There exists a q \u2208 [1, \u221e] and a continuous function C : R \u00d7 R M \u2192 R + with C (0, 0) = 0 such that", "cite_spans": [], "ref_spans": [], "section": "Preliminaries"}, {"text": "for all x \u2208 R N + and y \u2208 R M holds true. This will ensure that if we can control the compressibility and the noise, we can also control the estimation error and in particular decode every noiseless observation of S-sparse signals exactly.", "cite_spans": [], "ref_spans": [], "section": "Preliminaries"}, {"text": "Given a measurement matrix A \u2208 R M\u00d7N and a norm \u00b7 on R M we propose to define the decoder as following: Given y \u2208 R M set Q A (y) as any minimizer of", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "We call this problem non-negative least residual (NNLR). In particular, for \u00b7 = \u00b7 1 this problem is called non-negative least absolute deviation (NNLAD) and for \u00b7 = \u00b7 2 this problem is known as the non-negative least squares (NNLS) studied in [KJ18] . In fact, we can translate the proof techniques fairly simple. We just need to introduce the dual norm.", "cite_spans": [{"start": 243, "end": 249, "text": "[KJ18]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Main Results"}, {"text": "Definition 3.1. Let \u00b7 be a norm on R M . The norm \u00b7 * on R M defined by v * := sup u \u22641 v, u , is called dual norm to \u00b7 .", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Note that the dual norm is actually a norm. To obtain a recovery guarantee for NNLR we have certain requirements on the measurement matrix A. As for most other convex optimization problems in compressed sensing, we use a null space property.", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Definition 3.2. Let S \u2208 [N ], q \u2208 [1, \u221e) and \u00b7 be any norm on R M . Further let A \u2208 R M\u00d7N . Suppose there exists constants \u03c1 \u2208 [0, 1) and \u03c4 \u2208 [0, \u221e) such that", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Then, we say A has the \u2113 q -robust null space property of order S with respect to \u00b7 or in short A has the \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 and \u03c4 . \u03c1 is called stableness constant and \u03c4 is called robustness constant.", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "In order to deal with the non-negativity, we need A to be biased in a certain way. In [KJ18] this bias was guaranteed with the M + criterion.", "cite_spans": [{"start": 86, "end": 92, "text": "[KJ18]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Main Results"}, {"text": "Then we say A obeys the the M + criterion with vector t and constant \u03ba :", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Note that \u03ba is actually a condition number of the matrix with diagonal A T t and 0 else. Condition number numbers are frequently used in error bounds of numerical linear algebra. The general recovery guarantee is the following and similar results have been obtained in the matrix case in [JJ20] .", "cite_spans": [{"start": 288, "end": 294, "text": "[JJ20]", "ref_id": "BIBREF14"}], "ref_spans": [], "section": "Main Results"}, {"text": "Theorem 3.4 (NNLR Recovery Guarantee). Let S \u2208 [N ], q \u2208 [1, \u221e) and \u00b7 be any norm on R M with dual norm \u00b7 * . Further, suppose that A \u2208 R M\u00d7N obeys a) the \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 and \u03c4 and b) the M + criterion with vector t and constant \u03ba.", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "If \u03ba\u03c1 < 1, the following recovery guarantee holds true: For all x \u2208 R N + and y \u2208 R M any minimizer", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "obeys the bound", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "If q = 1, this bound can be improved to", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Proof. The proof can be found in Section 7.", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Certain random measurement matrices guarantee uniform bounds on \u03ba for fixed vectors t. In [KJ18, Theorem 12] , it was proven that if A m,n are all i.i.d. 0/1 Bernoulli random variables, A has M + criterion with t = (1, . . . , 1) T \u2208 R M and \u03ba \u2264 3 with high probability. This is problematic, since if \u03ba > 1 it might happen that \u03ba\u03c1 < 1 is not fulfilled anymore. Since the stableness constant \u03c1 (S \u2032 ) as a function of S \u2032 is monotonically increasing, the condition \u03ba\u03c1(S \u2032 ) < 1 might only hold if S \u2032 < S. If that is the case, there are vectors x \u2208 \u03a3 S that are being recovered by BPDN but not by NNLS! This is for instance the case of the matrix A = 1 0 1 0 1 1 , which has \u2113 1 -robust null space property of order 1 with stableness constant \u03c1 := 1 2 and M + criterion with \u03ba \u2265 2 for any possible choice of t. In particular, the vector x = (0, 0, 1) T is not necessarily being recovered by the NNLAD and the NNLS. Hence, it is crucial that the vector t is chosen to minimize \u03ba and ideally obeys the optimal \u03ba = 1. This motivates us to use random walk matrices of regular graphs since they obey exactly this. We will only consider random walk matrices and no biadjacency matrices. Note that we have made a slight abuse of notation. The term D-LRBG as a short form for D-left regular bipartite graph refers in our case to the random walk matrix A but not the graph itself. We omit this minor technical differentiation, for the sake of shortening the frequently used term random walk matrix of a D-left regular bipartite graph. Lossless expanders are bipartite graphs that have a low number of edges but are still highly connected, see for instance [Vad12, Chapter 4] . As a consequence their random walk matrices have good properties for compressed sensing. It is well known that random walk matrices of a (2S, D, \u03b8)-lossless expanders obey the \u2113 1 -RNSP of order S with respect to \u00b7 1 , see [FR13, Theorem 13.11 ]. The dual norm of \u00b7 1 is the norm \u00b7 \u221e and the M + criterion is easily fulfilled, since the columns sum up to one. From Theorem 3.4 we can thus draw the following corollary.", "cite_spans": [{"start": 90, "end": 96, "text": "[KJ18,", "ref_id": "BIBREF16"}, {"start": 97, "end": 108, "text": "Theorem 12]", "ref_id": null}, {"start": 1646, "end": 1653, "text": "[Vad12,", "ref_id": null}, {"start": 1654, "end": 1664, "text": "Chapter 4]", "ref_id": null}, {"start": 1890, "end": 1896, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 1897, "end": 1910, "text": "Theorem 13.11", "ref_id": null}], "ref_spans": [], "section": "Main Results"}, {"text": "Corollary 3.6 (Lossless Expander Recovery Guarantee). Let S \u2208 [N ], \u03b8 \u2208 0, 1 6 . If A \u2208 0, D \u22121 M\u00d7N is a random walk matrix of a (2S, D, \u03b8)-lossless expander, then the following recovery guarantee holds true: For all x \u2208 R N + and y \u2208 R M any minimizer x # of", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "obeys the bound", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Proof. By [FR13, Theorem 13 .11] A has \u2113 1 -RNSP with respect to \u00b7 1 with constants \u03c1 = 2\u03b8 1\u22124\u03b8 and \u03c4 = 1 1\u22124\u03b8 . The dual norm of the norm \u00b7 1 is \u00b7 \u221e . If we set t := (1, . . . , 1) T \u2208 R M we get", "cite_spans": [{"start": 10, "end": 16, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 17, "end": 27, "text": "Theorem 13", "ref_id": null}], "ref_spans": [], "section": "Main Results"}, {"text": "Hence, A has the M + criterion with vector t and constant \u03ba = 1 and the condition \u03ba\u03c1 < 1 is immediately fulfilled. We obtain t * = t \u221e = 1 and max n\u2208[N ] A T t \u22121 n = 1. Applying Theorem 3.4 with improved bound for q = 1 and these values yields", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "If we additionally substitute the values for \u03c1 and \u03c4 we get", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "This finishes the proof.", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "Note that, if M \u2265 2 \u03b8 exp 2 \u03b8 S ln eN ", "cite_spans": [], "ref_spans": [], "section": "Main Results"}, {"text": "If A is a random walk matrix of a (2S, D, \u03b8)-lossless expander with \u03b8 \u2208 0, 1 6 , then we can also draw a recovery guarantee for the NNLS. By [FR13, Theorem 13 .11] A has \u2113 1 -RNSP with respect to \u00b7 1 with constants \u03c1 = 2\u03b8 1\u22124\u03b8 and \u03c4 = 1 1\u22124\u03b8 and hence also \u2113 1 -RNSP with respect to \u00b7 2 with constants \u03c1 \u2032 = \u03c1 and \u03c4 \u2032 = \u03c4 M 1 2 . Similar to the proof of Corollary 3.6 we can use Theorem 3.4 to deduce that any minimizer x # of", "cite_spans": [{"start": 141, "end": 147, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 148, "end": 158, "text": "Theorem 13", "ref_id": null}], "ref_spans": [], "section": "On the Robustness Bound for Lossless Expanders"}, {"text": "obeys the bound", "cite_spans": [], "ref_spans": [], "section": "On the Robustness Bound for Lossless Expanders"}, {"text": "If the measurement error e = y \u2212 Ax is a constant vector, i.e. e = \u03b1\u00bd, then e 1 = M 1 2 e 2 . In this case the error bound of the NNLS is just as good as the error bound of the NNLAD. However, if e is a standard unit vector, then e 1 = e 2 . In this case the error bound of the NNLS is significantly worse than the error bound of the NNLAD. Thus, the NNLAD performs better under peaky noise, while the NNLS and NNLAD are tied under noise with evenly distributed mass. We will verify this numerically in Subsection 5.1. One can draw a complementary result for matrices with biased sub-Gaussian entries, which obey the \u2113 2 -RNSP with respect to \u00b7 2 and the M + criterion in the optimal regime [KJ18] . Table 1 states the methods, which have an advantage over the other in each scenario.", "cite_spans": [{"start": 691, "end": 697, "text": "[KJ18]", "ref_id": "BIBREF16"}], "ref_spans": [{"start": 700, "end": 707, "text": "Table 1", "ref_id": "TABREF0"}], "section": "On the Robustness Bound for Lossless Expanders"}, {"text": "Measurement Matrix D-LRBG (\u2113 1 ) biased sub-Gaussian (\u2113 2 ) peaky e 1 \u2248 e 2 NNLAD -Noise even mass e 1 \u2248 M 1 2 e 2 -NNLS unknown noise NNLAD NNLS ", "cite_spans": [], "ref_spans": [], "section": "On the Robustness Bound for Lossless Expanders"}, {"text": "For \u00b7 = \u00b7 p the NNLR is a convex optimization problem and the objective function has a simple and globally bounded subdifferential. Thus, the NNLR can directly be solved with a projective subgradient method using a problem independent step size. Such subgradient methods achieve only a convergence rate of O log (k) k \u2212 1 2 towards the optimal objective value [Nes04, Section 3.2.3], where k is the number of iterations performed. In the case that the norm is the \u2113 2 -norm, we can transfer the problem into a differentiable version, i.e. the NNLS", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Since the gradient of such an objective is Lipschitz, this problem can be solved by a projected gradient method with constant step size, which achieve a convergence rate of O k \u22122 towards the optimal objective value [BT09] [AP16] . However this does not generalize to the \u2113 1 -norm. We will show below that the case of the \u2113 1 -norm can be solved with the convergence rate O k \u22121 towards the optimal objective value by using the proximal point method proposed in [CP11] . This results in the following algorithm.", "cite_spans": [{"start": 223, "end": 229, "text": "[AP16]", "ref_id": "BIBREF1"}, {"start": 463, "end": 469, "text": "[CP11]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Algorithm 4.1 (NNLAD as First Order Method).", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "A convergence guarantee can be deduced from the following result.", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Then, the following statements hold true:", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "(1) The iterates and averages converge:", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "The sequences x k k\u2208N and x k k\u2208N converge to a minimizer of argmin z\u22650", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Az \u2212 y 1 .", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "(2) The iterates and averages are feasible:", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "There is a stopping criteria for the iterates:", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Az \u2212 y 1 .", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "(4) The stopping criteria also holds for the averages by replacing x k withx k and w k withw k .", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "(5) The averages obey the convergence rate to optimal objective value:", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "Proof. The proof can be found in Section 8 Note that Algorithm 4.1 yields a convergence guarantee for both the iterates and averages, but the convergence rate is only guaranteed for the averages. Algorithm 4.1 is optimized in the sense that it uses the least possible number of matrix vector multiplications per iteration, since these govern the computational complexity. ", "cite_spans": [], "ref_spans": [], "section": "NNLAD using a Proximal Point Method"}, {"text": "As stated the NNLS achieves the convergence rate O k \u22122 [AP16] while the NNLAD only achieves the convergence rate of O k \u22121 towards to optimal objective value. However, this should not be considered as weaker, since the objective function of the NNLS is the square of a norm. If x k are the iterates of the NNLS implementation of [AP16] , algebraic manipulation yields", "cite_spans": [{"start": 56, "end": 62, "text": "[AP16]", "ref_id": "BIBREF1"}, {"start": 330, "end": 336, "text": "[AP16]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "On the Convergence Rate"}, {"text": "Thus, the \u2113 2 -norm of the residual of the NNLS iterates only decays in the same order as the \u2113 1 -norm of the residual of the NNLAD averages.", "cite_spans": [], "ref_spans": [], "section": "On the Convergence Rate"}, {"text": "Let \u00b7 be a norm on R M . Since the exact optimal objective value inf z\u22650 Az \u2212 y is an unknown value, checking for an optimizer can be done only with Karush Kuhn Tucker conditions or by checking if zero is in the subdifferential. In particular, one needs to find a vector w \u2208 R M which satisfies the following conditions", "cite_spans": [], "ref_spans": [], "section": "Stopping Criteria"}, {"text": "where \u00b7 * is the dual norm of \u00b7 . The first two conditions ensure that A T w is in the subdifferential of z \u2192 Az \u2212 y for z = x and the second two conditions ensure that \u2212A T w is in the subdifferential of the characteristic function of R N + at x. If \u00b7 is the \u2113 p -norm for p \u2208 (1, \u221e) and Ax \u2212 y = 0, by Hoelder's inequality the vector", "cite_spans": [], "ref_spans": [], "section": "Stopping Criteria"}, {"text": "is the only vector, that satisfies (Opt 1) and (Opt 2). Checking (Opt 3) and (Opt 4) is then easy. However, if \u00b7 is the \u2113 1 or \u2113 \u221e -norm, the first two equations have multiple solutions and checking for an optimizer is a non-trivial feasibility problem. Statements (3) and (4) of Proposition 4.2 give us a simple condition to check for an optimizer, which can be done in O (M + N ) by a proper implementation.", "cite_spans": [], "ref_spans": [], "section": "Stopping Criteria"}, {"text": "The question arises whether or not it is better to estimate with averages or iterates. Numerical testing suggest that the iterates reach tolerance thresholds significantly faster than the averages. We can only give a heuristically explanation for this phenomenon. From statement (3) of Proposition 4.2 we obtain that lim k\u2192\u221e A T w k \u2265 0. In practice we observe that A T w k \u2265 0 for all sufficiently large k. However, A T w k+1 \u2265 0 yields x k+1 \u2264 x k . This monotonicity promotes the converges of the iterates and gives a clue why the iterates seem to converge better in practice. See Figure 5 and Figure 6 .", "cite_spans": [], "ref_spans": [{"start": 584, "end": 592, "text": "Figure 5", "ref_id": "FIGREF15"}, {"start": 597, "end": 605, "text": "Figure 6", "ref_id": "FIGREF16"}], "section": "Iterates or Averages"}, {"text": "In this section we will compare NNLAD with several state of the art recovery methods in terms of achieved sparsity levels and decoding time.", "cite_spans": [], "ref_spans": [], "section": "Numerical Experiments"}, {"text": "We recall that the goal is to recover x from the noisy linear measurements y = Ax+ e. To investigate properties of the minimizers of NNLAD we compare it to the minimizers of the well studied problems basis pursuit (BP), optimally tuned BPDN, optimally tuned \u2113 1 -constrained least residual (CLR) and the NNLS, which are given by argmin z: Az\u2212y 1 \u2264\u01eb", "cite_spans": [], "ref_spans": [], "section": "Properties of the NNLAD Optimizer"}, {"text": "argmin z:Az=y", "cite_spans": [], "ref_spans": [], "section": "Properties of the NNLAD Optimizer"}, {"text": "Further, we compare the NNLAD to any cluster point of the sequence of the expander iterative hard thresholding (EIHT) given by", "cite_spans": [], "ref_spans": [], "section": "Properties of the NNLAD Optimizer"}, {"text": "where median (z) n is the median of (z m ) m\u2208Row({n}) , and P \u03a3S (v) is a hard thresholding operator, i.e. any minimizer of argmin z\u2208\u03a3S 1 2 z \u2212 v 2 2 . By convex decoders we refer to BPDN, BP, CLR, NNLAD, and NNLS. We choose the optimal tuning \u01eb = e 1 for the BPDN and \u03c4 = x 1 for the CLR. Thus, such optimally tuned BPDN and CLR are representing a best case benchmark. In [K19, Figure 1 .1] it was noticed that tuning the BPDN with \u01eb > e p often leads to worse estimation errors than tuning with \u01eb < e p for p = 2. Thus, BP is representing a worst case benchmark, as a version of BPDN with no prior knowledge about a noise. At the moment we do not care about the method to calculate the minimizers of the optimization problems, thus we solve all optimization problems with the CVX package of Matlab [GB14] , [MS08] . For a given SN R, r, N, M, D, S we will do the following experiment multiple times:", "cite_spans": [{"start": 800, "end": 806, "text": "[GB14]", "ref_id": "BIBREF12"}, {"start": 809, "end": 815, "text": "[MS08]", "ref_id": "BIBREF19"}], "ref_spans": [{"start": 379, "end": 387, "text": "Figure 1", "ref_id": "FIGREF6"}], "section": "Properties of the NNLAD Optimizer"}, {"text": "4. Define the observation y := Ax + e.", "cite_spans": [], "ref_spans": [], "section": "Generate a noise e uniformly at random from"}, {"text": "For each decoder Q A calculate an estimator x # := Q A (y) and collect the relative estimation error", "cite_spans": [], "ref_spans": [], "section": "5."}, {"text": "In this experiment we have SN R = Ax 1 e 1 and since A is a D-LRBG and x \u2265 0, we further have Ax 1 = x 1 = 1. Note that for r = 0 and r = 1 we obtain two different noise distributions. If e is uniformly distributed on S M\u22121 1 , then the absolute value of each component |e m | is a random variable with density h \u2192", "cite_spans": [], "ref_spans": [], "section": "5."}, {"text": "M+1 . By testing one can observe a concentration around this expected value, in particular that M , then e 2 = e 1 . Thus, these two noise distributions each represent randomly drawn noise vectors obeying one norm equivalence asymptotically tightly up to a constant. From (1) and (2) we expect that the NNLS has roughly the same estimation errors as the NNLAD for r = 1, i.e. the evenly distributed noise, and significantly worse estimation errors for r = 0, i.e. the peaky noise.", "cite_spans": [], "ref_spans": [], "section": "5."}, {"text": "We fix the constants r = 1, N = 1024, M = 256, D = 10, SN R = 1000 and vary the sparsity level S \u2208 [64]. For each S we repeat Experiment 1 100 times. We plot the mean of the relative \u2113 1 -estimation error and the mean of the logarithmic relative \u2113 1 -estimation error, i.e.", "cite_spans": [], "ref_spans": [], "section": "Quality of the Estimation Error for Varying Sparsity"}, {"text": "over the sparsity. The result can be found in Figure 1a and Figure 1b . For S \u2265 30 the estimation error of the EIHT randomly peaks high. We deduce that the EIHT fails to recover the signal reliably for S \u2265 30, while the NNLAD and other convex decoders succeed. This is not surprising, since by [FR13, Theorem 13 .15] the EIHT obeys a robust recovery guartanee for S-sparse signals, whenever A is the random wak matrix of a (3S, D, \u03b8 \u2032 )-lossless expander with \u03b8 \u2032 < 1 12 . This is significantly stronger than the (2S, D\u03b8)-lossless expander property with \u03b8 < 1 6 required for a null space property. However, if the EIHT recovers a signal, it recovers it significantly better than any convex method. This might be the case, since the originally generated signal is indeed from \u03a3 S , which is being enforced by the hard thresholding of the EIHT, but not by the convex decoders. This suggests that it might be useful to consider using thresholding on the output of any convex decoder to increase the accuracy if the orignal signal is indeed sparse and not only compressible. For the remainder of this subsection we focus on convex decoders. Contrary to our expectation the BPDN achieves worse estimation errors than all other convex decoders for S \u2265 60, even worse than the not tuned BP. The authors have no explanation for this phenomenon. Apart from that we observe that the CLR and BP indeed perform as respectively best and worst case benchmark. However, the difference between BP and CLR becomes rather small for high S. We deduce that tuning becomes less important near the optimal sampling rate. The NNLAD, NNLS and CLR perform roughly the same. This is quite strong, since BPDN and CLR are optimally tuned using unknown prior information. As expected the NNLS performs roughly the same as the NNLAD, see Table 1 . However, this is the result of the noise distribution for r = 1. We repeat Experiment 1 with the same constants, but r = 0, i.e. e is a unit vector scaled by \u00b1 Ax 1 SN R . We plot the mean of the relative \u2113 1 -estimation error and the mean of the logarithmic relative \u2113 1 -estimation error over the sparsity. The result can be found in Figure 2a and Figure 2b . We want to note that similarly to Figure 1a the EIHT works only unreliably for S \u2265 30. Even though the mean of the logarithmic relative \u2113 1 -estimation error of NNLS is worse than the one of EIHT for 30 \u2264 S \u2264 60, the NNLS does not fail but only approximates with a weak error bound. As the theory suggests, the NNLS performs significantly worse than the NNLAD, see Table 1 . It is worth to mention, that the estimaton errors of NNLS seem to be bounded by the estimation errors of BP. This suggests that A obeys a \u2113 1 quotient property, that bounds the estimation error of any instance optimal decoder, see [FR13, Lemma 11 .15].", "cite_spans": [{"start": 294, "end": 300, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 301, "end": 311, "text": "Theorem 13", "ref_id": null}, {"start": 2786, "end": 2792, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 2793, "end": 2801, "text": "Lemma 11", "ref_id": null}], "ref_spans": [{"start": 46, "end": 55, "text": "Figure 1a", "ref_id": "FIGREF6"}, {"start": 60, "end": 69, "text": "Figure 1b", "ref_id": "FIGREF6"}, {"start": 1808, "end": 1815, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 2154, "end": 2163, "text": "Figure 2a", "ref_id": "FIGREF8"}, {"start": 2168, "end": 2177, "text": "Figure 2b", "ref_id": "FIGREF8"}, {"start": 2214, "end": 2223, "text": "Figure 1a", "ref_id": "FIGREF6"}, {"start": 2545, "end": 2552, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Mean (N\u2113"}, {"text": "Theorem 3.4 states that the NNLAD has an error bound similarly to the optimally tuned CLR and BPDN. Further, by (1) the ratio The logarithmic relative \u2113 1 -estimation errors of the different decoders stay in a constant relation to each other over the whole range of SN R. This relation is roughly the relation we can find in Figure 1b for S = 32. As expected the the ratio of relative \u2113 1 -estimation error and \u2113 1 -noise power stays constant independent on the SN R for all decoders. We deduce that the NNLAD is noise-blind. We repeat the experiment with r = 0 and obtain Figure 4a and Figure 4b . Against our expectation,", "cite_spans": [], "ref_spans": [{"start": 325, "end": 334, "text": "Figure 1b", "ref_id": "FIGREF6"}, {"start": 573, "end": 582, "text": "Figure 4a", "ref_id": "FIGREF13"}, {"start": 587, "end": 596, "text": "Figure 4b", "ref_id": "FIGREF13"}], "section": "Noise-Blindness"}, {"text": "and not", "cite_spans": [], "ref_spans": [], "section": "Noise-Blindness"}, {"text": "x\u2212x # 1", "cite_spans": [], "ref_spans": [], "section": "Noise-Blindness"}, {"text": "x 1 e 1 seems to be constant. Since", "cite_spans": [], "ref_spans": [], "section": "Noise-Blindness"}, {"text": "fairly small, we suspect that this is the result of CVX reaching a tolerance parameter 1 \u221a eps \u2248 1.5 \u00b7 10 \u22128 and terminating, while the actual optimizer might in fact be the original signal. It is definitely note worthy that even with the incredibly small signal to noise ration of 10 the signal can be recovered by the NNLAD with an estimation error of 1.0 \u00b7 10 \u22127 for this noise distribution.", "cite_spans": [], "ref_spans": [], "section": "Noise-Blindness"}, {"text": "To investigate the convergence rates of the NNLAD as proposed in Proposition 4.2, we compare it to different types of decoders when e = 0. As a best case benchmark, we consider the EIHT, which has a linear convergence rate O c \u2212k towards the signal [FR13, Theorem 13.15]. As a direct competitor we consider the NNLS implemented by the methods of [AP16] 2 , which has a convergence rate of O k \u22122 towards the optimal objective value. [AP16] can also be used to calculate the LASSO. However calculating the projection onto the \u2113 1 -ball in R N , is computationally slightly more complex than the projection onto R N + . Thus the NNLS will also be a lower bound for the LASSO. As a worst case benchmark we consider a simple projected subgradient implementation of NNLAD using the Polyak step size, i.e.", "cite_spans": [{"start": 346, "end": 352, "text": "[AP16]", "ref_id": "BIBREF1"}, {"start": 433, "end": 439, "text": "[AP16]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "NNLAD vs iterative methods"}, {"text": "which has a convergence rate of O k \u2212 1 2 towards the optimal objective value [Pol87, Section 7.2.2 & Section 5.3.2] [Boy14, Section 6]. We will always initialize all iterated methods by zero vectors. The EIHT will always use the parameter S \u2032 = x 0 , the NNLAD \u03c3 = \u03c4 = 0.99 A \u22121 2\u21922 and the NNLS the parameters s = 0.99 A \u22122 2\u21922 and \u03b1 = 3.01, see [AP16] . Parameters that can be computed from A, will be calculated before the timers start. This includes the adjacency structure of A for the EIHT, \u03c3, \u03c4 for NNLAD, s, \u03b1 for NNLS, since these are considered to be a part of the decoder. We will do the following experiment multiple times: For r = 2 this represents a biased sub-gaussian random ensemble [KJ18] with optimal recovery guarantees for the NNLS. For r = 1 this represents a D-LRBG random ensemble with optimal recovery guarantees for the NNLAD. We fix the constants r = 1, N = 1024, M = 256, S = 16, D = 10 and repeat Experiment 2 100 times. We plot the mean of the logarithmic relative \u2113 1 -estimation error and the mean of the relative \u2113 1 -norm of the residual, i.e.", "cite_spans": [{"start": 348, "end": 354, "text": "[AP16]", "ref_id": "BIBREF1"}, {"start": 701, "end": 707, "text": "[KJ18]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "NNLAD vs iterative methods"}, {"text": "Mean (LN\u2113 1 E) = Mean 10 log 10", "cite_spans": [], "ref_spans": [], "section": "NNLAD vs iterative methods"}, {"text": "x k \u2212 x 1 x 1 and Mean (LN\u2113 1 R) = Mean 10 log 10 Ax k \u2212 y 1 y 1 over the sparsity and the time. The result can be found in Figure 5 and Figure 6 . The averages of NNLAD converge significantly slower than the iterates, even though we lack a convergence rate for the iterates. We deduce that one should always use the iterates of NNLAD to recover a signal. Surprisingly, the averages converge even slower than the subgradient method. However, this is not because the averages converge slow, but rather because the subgradient method and all others converges faster than expected. In particular, the NNLAD iterates, EIHT and the NNLS all converge linearly towards the optimal objective value and towards the signal. Even the subgradient method converges almost linearly. We deduce that the NNLS is the fastest method to recover a signal if A is a D-LRBG. Apart from a constant the NNLAD iterates, EIHT and NNLS converge in the same order. However, this behavior does not hold if we consider a different distribution for A as one can verify by setting each component A m,n as independent 0/1 Bernoulli random variables. While EIHT has better iterations compared to the NNLS, it still takes more time to achieve the same estimation errors and residuals. We plot the mean of the time required to calculate the first k iterations in Figure 7 . The EIHT requires roughly 6 times as long as any other method to calculate each iteration. All methods but the EIHT can be implemented with only two matrix vector multiplications, namely once by A and once by A T . Both of these requires roughly 2DN floating point operations. Hence each iterations require O (4DN ) floating point iterations. The EIHT only calculates one matrix vector multiplication, but also the median. This calculation is significantly slower than a matrix vector multiplication. For every n \u2208 [N ] we need to order a vector with D elements, which can be performed in O (D log (D)). Hence, each iteration of EIHT requires O (DN log (D)) floating point operations, which explains why the EIHT requires significantly more time for each iteration. As we have seen the NNLS is able to recover signals faster than any other method, however it also only obeys sub-optimal robustness guarantees as we have seen in Figure 4a . We ask ourself whether or not the NNLS is also faster with a more natural measurement scheme, i.e. if A m,n are independent 0/1 Bernoulli random variables. We repeat Experiment 2 100 times with r = 2 for the NNLS and r = 1 for the other methods. We again plot the mean of the logarithmic relative \u2113 1 -estimation error and the mean of the relative \u2113 1 -norm of the residual in Figure 8 and Figure 9 . The NNLAD and the EIHT converge to the solution with roughly the same time. Even the subgradient implementation of the NNLAD recovers a signal in less time than the NNLS. Further the convergence of NNLS does not seem to be linearly anymore. We deduce that sparse structure of A has a more significant influence on the decoding time than the smoothness of the data fidelity term. Also we deduce that even the subgradient method is a viable choice to recover a signal.", "cite_spans": [], "ref_spans": [{"start": 124, "end": 132, "text": "Figure 5", "ref_id": "FIGREF15"}, {"start": 137, "end": 145, "text": "Figure 6", "ref_id": "FIGREF16"}, {"start": 1327, "end": 1335, "text": "Figure 7", "ref_id": "FIGREF17"}, {"start": 2266, "end": 2275, "text": "Figure 4a", "ref_id": "FIGREF13"}, {"start": 2655, "end": 2663, "text": "Figure 8", "ref_id": "FIGREF18"}, {"start": 2668, "end": 2676, "text": "Figure 9", "ref_id": "FIGREF19"}], "section": "NNLAD vs iterative methods"}, {"text": "As a last test we compare the NNLAD to the SPGL1 [vdBF09] [vdBF19] toolbox for matlab. Experiment 3.", "cite_spans": [{"start": 49, "end": 57, "text": "[vdBF09]", "ref_id": "BIBREF28"}], "ref_spans": [], "section": "NNLAD vs SPGL1"}, {"text": "1. Generate the measurement matrix A \u2208 0, D \u22121 M\u00d7N as a uniformly at random drawn D-LRBG.", "cite_spans": [], "ref_spans": [], "section": "NNLAD vs SPGL1"}, {"text": "3. Define the observation y := Ax.", "cite_spans": [], "ref_spans": [], "section": "Generate the signal x uniformly at random from"}, {"text": "x 2 and the time to calculate x # .", "cite_spans": [], "ref_spans": [], "section": "Use a benchmark decoder to calculate an estimator x # and collect the relative estimation errors"}, {"text": "Collect the time to perform these iterations. If this threshold can not be reached after 10 5 iterations, the recovery failed and the time is set to \u221e.", "cite_spans": [], "ref_spans": [], "section": "For each iterative method calculate iterations until"}, {"text": "We again fix the dimension N = 1024, M = 256, D = 10 and vary S \u2208 [128]. For both the BP implementation of SPGL1 and the LASSO implementation of SPGL1 we repeat Experiment 3 100 times for each S. We plot the mean of the time to calculate the estimators and plot these over the sparsity in Figure 10a and Figure 10b . The NNLAD implementation is slower than both SPGL1 methods for small S. However, if we have the optimal number of measurements M \u2208 O S log N S , the NNLAD is faster than both SPGL1 methods.", "cite_spans": [], "ref_spans": [{"start": 289, "end": 299, "text": "Figure 10a", "ref_id": "FIGREF6"}, {"start": 304, "end": 314, "text": "Figure 10b", "ref_id": "FIGREF6"}], "section": "For each iterative method calculate iterations until"}, {"text": "The implementation of NNLAD as presented in Algorithm 4.1 is a reliable recovery method for sparse nonnegative signals. There are methods that might be faster, but these either recover a smaller number of coefficients (EIHT, greedy methods) or they obey sub-optimal recovery guarantees (NNLS). The implementation is as fast as the commonly uses SPGL1 toolbox, but has the advantage that it requires no tuning depending on the unknown x or e. Lastly, the NNLAD can handle peaky noise overwhelmingly good.", "cite_spans": [], "ref_spans": [], "section": "Summary"}, {"text": "With the outbreak and rapid spread of the COVID-19 virus, we are in the need of testing a lot of people for an infection. Since we can only test a fixed number of persons in a given time, the number of persons tested for the virus grows at most linearly. On the other hand, models suggest that the number of possibly infected persons grows exponentially. At some point, if that is not already the case, we will have a shortage of test kits and we will not be able to test every person. It is thus desirable, to test as much persons with as few as possible test kits. The field group testing develops strategies to test groups of individuals instead of individuals in order to reduce the amount of tests required to identify individuals with a certain property. The first advances in group testing were made in [Dor43] . For a general overview about group testing we refer to [AJS19] . The problem of testing a large group for a virus can be modeled as a compressed sensing problem in the following way: Suppose we want to test where e pro m is the amount of viruses in the sample originating from a possible contamination of the sample. Usually, when using a test kit the viruses are duplicated multiple times, for instance with a polymerase chain reaction. The test kit will then detect a known, bijective transformation of each individual component y m . Hence, we can calculate y m directly and thus assume that y m is the result of the test. After all M tests we detect the quantity", "cite_spans": [{"start": 810, "end": 817, "text": "[Dor43]", "ref_id": "BIBREF8"}, {"start": 875, "end": 882, "text": "[AJS19]", "ref_id": "BIBREF0"}], "ref_spans": [], "section": "Application for Group Testing"}, {"text": "where e = Ae spec + e pro . For now we assume that e is peaky in terms of Table 1 , and we will later argue why such a model is natural. Often each specimen is tested separately, meaning that A is the identity. In particular, we need at least as much test kits as specimens. Further, we estimate the true quantitiy of viruses x n by x # n := y n , which results in the estimation error x # n \u2212 x n = e n = e spe n + e pro n . In this scenario the estimation error for the n-th person is only affected by the errors made while taking its specimen and testing its sample. Since the noise vector e is peaky, some but few tests will be inaccurate and might result in false positives or false negatives. In general, only a fraction of persons is indeed affected by the virus. Thus, we assume that x 0 \u2264 S for some small S. Since the amount of viruses is a non-negative value, we also have x \u2265 0. Corollary 3.6 suggests to choose A as the random walk matrix of a lossless expander or by [FR13, Theorem 13 .7] to choose A as a uniformly at random chosen D-LRBG. Such a matrix A has non-negative entries and the column sums of A are not greater than one. This is a necessary requirement since each column sum is the total amount of specimen used in the test procedure. Especially, a fraction of D \u22121 of each specimen is used in exactly D test kits. In order to calculate an estimation to the true quantity of viruses x we propose to use the NNLAD. By Corollary 3.6 and [FR13, Theorem 13.7] this allows us to reduce the number of test kits required to M \u2248 CS log e N S . Further, as we have seen in Figure 4a and Figure 4b , the approximation error is incredibly small since the noise e is peaky. In this manner, the estimation will even be more exact than by testing each specimen separately. It remains to argue, why a peaky noise model is natural for the problem of testing a large group for a virus. In general, the sample of the m-th test kit will be affected by the noise e pro m = 0 if for instance the sample is contaminated by a specimen of a different person or a laboratory employee. Due to the caution and expertise of members in the health care system, this is a rare occasion, but when it happens the effect will be rather strong. Thus, it is natural to assume that e pro is peaky. Similarly, the specimen of the n-th person is affected by the noise e spe n under similar circumstances. Thus, it is also natural to assume that e spe is peaky. If we test each specimen seperately it follows that e = e spe + e pro is peaky. On the other hand if A is a D-LRBG, the sample of the m-th test kit is affected by the noise of the n-th specimen if and only if A m,n = 0. Since, there are exactly D non-zeros per column, exactly D samples are affected by the noise of the n-th specimen, i.e. by D \u22121 e spe n . If n e denotes the number of components of e spe with significant absolute value and m e denotes the number of components of e pro with significant absolute value, then the number of components of e with significant absolute value is at most Dn e + m e . If D is sufficiently small, as it is required for [FR13, Theorem 13 .7], the noise will be peaky. Note that the lack on knowledge of the noise e favors the NNLAD recovery method over a (BPDN) approach. Further, since the total sum of viruses in all patients given by n\u2208 [N ] x n = x 1 is unknown, it is undesirable to use (CLR).", "cite_spans": [{"start": 981, "end": 987, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 988, "end": 998, "text": "Theorem 13", "ref_id": null}, {"start": 3111, "end": 3117, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 3118, "end": 3128, "text": "Theorem 13", "ref_id": null}, {"start": 3331, "end": 3335, "text": "[N ]", "ref_id": null}], "ref_spans": [{"start": 74, "end": 81, "text": "Table 1", "ref_id": "TABREF0"}, {"start": 1590, "end": 1599, "text": "Figure 4a", "ref_id": "FIGREF13"}, {"start": 1604, "end": 1613, "text": "Figure 4b", "ref_id": "FIGREF13"}], "section": "Application for Group Testing"}, {"text": "By \u00bd we denote the all ones vector in R N or R M respectively. The proof is an adaption of the steps used in [KJ18] . As for most convex optimization problems in compressed sensing we require [FR13, Theorem 4 .25] and [FR13, Theorem 4 .20] respectively. . Let q \u2208 [1, \u221e) and suppose A has the \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 and \u03c4 . Then, it holds that", "cite_spans": [{"start": 109, "end": 115, "text": "[KJ18]", "ref_id": "BIBREF16"}, {"start": 192, "end": 198, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 199, "end": 208, "text": "Theorem 4", "ref_id": null}, {"start": 218, "end": 224, "text": "[FR13,", "ref_id": "BIBREF11"}, {"start": 225, "end": 234, "text": "Theorem 4", "ref_id": null}], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "If q = 1, this bound can be improved to", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Note that by a modification of the proof this result also holds for q = \u221e. [PJ20] . As a consequence, all our statements also hold for q = \u221e with 1 q := 0. If W \u2208 R N \u00d7N is a diagonal matrix, we can calculate some operator norms fairly easy:", "cite_spans": [{"start": 75, "end": 81, "text": "[PJ20]", "ref_id": "BIBREF21"}], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "|W n,n | for all q \u2208 [1, \u221e] .", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "We use this relation frequently over this section. Furthermore, we use [KJ18, Lemma 5] without adaption. For the sake of completeness we add a short proof.", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Lemma 7.2 ( [KJ18, Lemma 5] ). Let q \u2208 [1, \u221e) and suppose that A \u2208 R M\u00d7N has \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 and \u03c4 . Let W \u2208 R N \u00d7N be a diagonal matrix with W n,n > 0. If \u03c1 \u2032 = W q\u2192q W \u22121 1\u21921 \u03c1 < 1, then AW \u22121 has \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 \u2032 = W q\u2192q W \u22121 1\u21921 \u03c1 and \u03c4 \u2032 = W q\u2192q \u03c4 .", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Proof. Let v \u2208 R N and # (T ) \u2264 S. If we apply the RNSP of A for the vector W \u22121 v T , we get", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "This finishes the proof.", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Next we adapt [KJ18, Theorem 4] to account for arbitrary norms. Further, we obtain a slight improvement in form of the dimensional scaling constant S 1 q \u22121 . With this, our error bound becomes for S \u2192 \u221e asymptotically the error bound of the basis pursuit denoising, whenever \u03ba = 1 and q > 1 [FR13] .", "cite_spans": [{"start": 292, "end": 298, "text": "[FR13]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Proposition 7.3 ( Similar to [KJ18, Theorem 4] ). Let q \u2208 [1, \u221e) and \u00b7 be a norm on R M with dual norm \u00b7 * . Suppose A has \u2113 q -RNSP of order S with respect to \u00b7 with constants \u03c1 and \u03c4 . Suppose A has the M + criterion with vector t and constant \u03ba and that \u03ba\u03c1 < 1. Then, we have", "cite_spans": [{"start": 29, "end": 35, "text": "[KJ18,", "ref_id": "BIBREF16"}, {"start": 36, "end": 46, "text": "Theorem 4]", "ref_id": null}], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "If q = 1, this bound can be improved to", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Proof. Let x, z \u2265 0. In order to apply Lemma 7.2 we set W as the matrix with diagonal A T t and zero else. It follows that W n,n > 0 and W q\u2192q W \u22121 1\u21921 \u03c1 = \u03ba\u03c1 < 1. We can apply Lemma 7.2, which yields that AW \u22121 has \u2113 q -RNSP with constants \u03c1 \u2032 = W q\u2192q W \u22121 1\u21921 \u03c1 = \u03ba\u03c1 and \u03c4 \u2032 = W q\u2192q \u03c4 = max n\u2208[N ] A T t n \u03c4 . We apply Theorem 7.1 with the matrix AW \u22121 , the vectors Wx, Wz and the constants \u03c1 \u2032 and \u03c4 \u2032 and get", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "We lower bound the left hand side further to get", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "We want to estimate the term Wz 1 \u2212 Wx 1 using the M + criterion. Since z, x \u2265 0, W n,n = A T t n > 0 and W is a diagonal matrix, we have", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Applying this to (4) we get", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "If q = 1 we can repeat the proof with the improved bound of Theorem 7.1.", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "After these auxiliary statements it remains to prove the main result of Section 3 about the properties of the NNLR minimizer.", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "Proof of Theorem 3.4. By applying Proposition 7.3 with x and z := x # \u2265 0 we get", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "(1 + \u03ba\u03c1)", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "where in the last step we used that x # is a minimizer and x is feasible. If q = 1 we can repeat the proof with the improved bound of Proposition 7.3.", "cite_spans": [], "ref_spans": [], "section": "Proof of NNLR Recovery Guarantee"}, {"text": "In this section we only care about minimizers of argmin z\u22650 Az \u2212 y 1 , and thus x will denote any element in R N and does not need to be sparse or compressible. In order to prove Proposition 4.2 we introduce saddle point problems and technical notations from optimization.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "holds true, then x # , w # is called saddle point of f . In general we have for any point (", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "This yields that the inequality", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "holds true, but not necessarily with equality. The equality is a condition of the existence of a saddle point. The", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "\u2265 0 is called the duality gap. Further, (5) and (6) yield the logical statement", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Given a function F :", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "The fenchel conjugate has several interesting properties, however we only require that if F is proper, convex and lower semicontinuous 3 , then also F * is proper, convex and lower semicontinuous and F * * = F holds true [Roc70, Theorem 12.2]. Given a proper, convex, lower-semicontinuous function F : R N \u2192 R, the proximal point operator of F is the function Prox F (\u00b7) : Then, the function f (x, w) := Ax, w + G (x) \u2212 F * (w) has a saddle point. Further, let \u03c4, \u03c3 \u2208 (0, \u221e) be parameters with \u03c3\u03c4 < A \u22122 2\u21922 and x 0 \u2208 R N , w 0 \u2208 R M be initializations. Set v 0 := x 0 and for all k \u2208 N 0 inductively w k+1 =Prox \u03c3F * w k + \u03c3Av k (PP 1)", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "The sequence x k , w k converges to a saddle point of f . Lastly, for any bounded sets", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "By a proper choice of F and G any saddle point of f will also give a minimizer of NNLAD. We denote this proper choice in the next lemma. Then F, G, F * , G * are proper, convex and lower semicontinuous. F * and G * are given by", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Proof. From the definition it is clear that F, G are proper, convex and lower semicontinuous. Hence F * and G * are also proper, convex and lower semicontinuous. By a direct calculation we have", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "For the other fenchel conjugate we calculate", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "where in the last step we used that each summand depends on exactly one component of w * . Now w m w * \u2212 |w * | is larger for sgn (w * ) = sgn (w m ), than for sgn (w * ) = sgn (w m ). Hence, we can restrict the supremum to the case sgn (w * ) = sgn (w m ) and obtain", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Since F is proper, convex and lower semicontinuous, we have F * * = F . Thus,", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "And lastly we have", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "which finishes the proof. For the sake of completeness we added a proof. ", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "and hence \u03c3 \u22121 (w \u2032 m \u2212 w m ) \u2208 [\u22121, 1]. It follows that zero is a possible subgradient, i.e. the subdifferential contains zero. Hence, w \u2032 is the unique minimizer. To prove (9), we apply the first statement to calculate", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": ".", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "It follows that By Lemma 8.2 F, G, F * , G * are proper, convex and lower-semicontinuous. Thus, the requirements of Theorem 8.1 are fulfilled, which yields that f has a saddle point and thus the duality gap is zero. By Lemma 8.2 and the fact that the duality gap is zero, it follows that", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Ax \u2212 y 1 and w # \u2208 argmax w\u2208R M :A T w\u22650, w \u221e \u22641 \u2212 w, y .", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "If (x \u2032 , w \u2032 ) are any points with x \u2032 \u2265 0, w \u2032 \u221e \u2264 1 and A T w \u2032 \u2265 0, then we have by Lemma 8.2", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Ax \u2032 \u2212 y 1 + y, w \u2032 = sup", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "If this is non-positive, (7) and (10) yield that x \u2032 is a minimizer of NNLAD. Hence, it holds true that Ax \u2032 \u2212 y 1 + y, w \u2032 \u2264 0 and x \u2032 \u2265 0 and w \u2032 \u221e \u2264 1 and A T w \u2032 \u2265 0 \u21d2 x \u2032 is minimizer of NNLAD. (11) Lastly, by Lemma 8.3 the iterates calculated in (iter 1), (iter 2) and (iter 3) are exactly the iterates calculated in (PP 1), (PP 2) and (PP 3) respectively. We will now prove all statements. By Theorem 8.1 the sequence x k , w k converges to some saddle point x # , w # . Hence, x k converges to x # , which is a minimizer of NNLAD by (10). Since any sequence of averages converges to the same value as the original sequence, statement (1) follows. Since x k and w k are in the image of the proximal point operator of G and F * respectively, they need to obey G x k < \u221e and F * w k < \u221e. Lemma 8.2 yields the x k \u2265 0 and w k \u221e \u2264 1. By convexity we obtain als\u014d x k \u2265 0 and w k \u221e \u2264 1. Statement (2) is proven. By Theorem 8.1 the sequence x k , w k converges to some saddle point x # , w # . By taking the limit, statement (2) yields x # \u2265 0 and w # \u221e \u2264 1. The saddle point property and Lemma 8.2 implies", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "By Lemma 8.2 again, this is only possible if w # is feasible, i.e.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "A T w # \u2265 0.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Hence, lim k\u2192\u221e A T w k \u2265 0 follows. By Lemma 8.2 and the feasibility of x # and w # we have lim k\u2192\u221e Ax k \u2212 y 1 + y, w k = Ax # \u2212 y 1 + y, w # = sup", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "which is zero, since x # , w # is a saddle point. This yields the convergence in statement (3). Since any sequence of averages converges to the same value as the original sequence, we also get the convergence of statement (4). The in particular part of statements (3) and (4) follows from (11) and statement (2). Hence, statements (3) and (4) are proven.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "To prove the the remaining statement (5) we choose B 1 := x # and B 2 := {w : w \u221e \u2264 1}. The bound of Theorem 8.1 becomes", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "w \u2212 w 0 2 2 = 1 k 1 2\u03c3", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "x # \u2212 x 0 2 2 + 1 2\u03c4 w 0 2 2 + 2 w 0 1 + M .", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "By using Lemma 8.2 and the feasibility of x # we get", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Now letw be a maximizer of sup w\u2208R M f x k , w . By statement (2) we get G x k = 0 and thusw is also a minimizer of the convex function w \u2192 \u2212Ax k , w + F * (w). Hence, the subdifferential of this function needs to contain zero atw. Since \u2202F * (w) = \u2205 whenever w \u221e > 1, we get w \u221e \u2264 1. This together with the feasibility ofx k yields", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "Combining (13), (14) and (15) yields", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "x # \u2212 x 0 2 2 + 1 2\u03c4 w 0 2 2 + 2 w 0 1 + M and finishes the proof.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}, {"text": "We want to remark that the other feasibility assumptions A T w k \u2265 0 and A Twk \u2265 0 does not need to hold.", "cite_spans": [], "ref_spans": [], "section": "Proof of Convergence Guarantee"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Group testing: An information theory perspective. Foundations and Trends in Communications and Information Theory", "authors": [{"first": "Matthew", "middle": [], "last": "Aldridge", "suffix": ""}, {"first": "Oliver", "middle": [], "last": "Johnson", "suffix": ""}, {"first": "Jonathan", "middle": [], "last": "Scarlett", "suffix": ""}], "year": 2019, "venue": "", "volume": "15", "issn": "", "pages": "196--392", "other_ids": {"DOI": ["10.1561/0100000099"]}}, "BIBREF1": {"ref_id": "b1", "title": "The rate of convergence of nesterov's accelerated forwardbackward method is actually faster than 1/k 2", "authors": [{"first": "Hedy", "middle": [], "last": "Attouch", "suffix": ""}, {"first": "Juan", "middle": [], "last": "Peypouquet", "suffix": ""}], "year": 2016, "venue": "SIAM Journal on Optimization", "volume": "26", "issn": "3", "pages": "1824--1834", "other_ids": {"DOI": ["10.1137/15M1046095"]}}, "BIBREF2": {"ref_id": "b2", "title": "On the uniqueness of non-negative sparse & redundant representations", "authors": [{"first": "Alfred", "middle": ["M"], "last": "Bruckstein", "suffix": ""}, {"first": "Michael", "middle": [], "last": "Elad", "suffix": ""}, {"first": "Michael", "middle": [], "last": "Zibulevsky", "suffix": ""}], "year": 2008, "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings", "volume": "", "issn": "", "pages": "5145--5148", "other_ids": {"DOI": ["10.1109/ICASSP.2008.4518817"]}}, "BIBREF3": {"ref_id": "b3", "title": "Subgradient Methods, Notes for EE364b", "authors": [{"first": "Stephen", "middle": [], "last": "Boyd", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "authors": [{"first": "Amir", "middle": [], "last": "Beck", "suffix": ""}, {"first": "Marc", "middle": [], "last": "Teboulle", "suffix": ""}], "year": 2009, "venue": "SIAM Journal on Imaging Sciences", "volume": "2", "issn": "1", "pages": "183--202", "other_ids": {"DOI": ["10.1137/080716542"]}}, "BIBREF5": {"ref_id": "b5", "title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "authors": [{"first": "Antonin", "middle": [], "last": "Chambolle", "suffix": ""}, {"first": "Thomas", "middle": [], "last": "Pock", "suffix": ""}], "year": 2011, "venue": "Journal of Mathematical Imaging and Vision", "volume": "40", "issn": "1", "pages": "120--145", "other_ids": {"DOI": ["10.1007/s10851-010-0251-1"]}}, "BIBREF6": {"ref_id": "b6", "title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information", "authors": [{"first": "E", "middle": ["J"], "last": "Candes", "suffix": ""}, {"first": "J", "middle": [], "last": "Romberg", "suffix": ""}, {"first": "T", "middle": [], "last": "Tao", "suffix": ""}], "year": 2006, "venue": "IEEE Transactions on Information Theory", "volume": "52", "issn": "2", "pages": "489--509", "other_ids": {"DOI": ["10.1109/TIT.2005.862083"]}}, "BIBREF7": {"ref_id": "b7", "title": "Compressed sensing", "authors": [{"first": "D", "middle": ["L"], "last": "Donoho", "suffix": ""}], "year": 2006, "venue": "IEEE Transactions on Information Theory", "volume": "52", "issn": "4", "pages": "1289--1306", "other_ids": {"DOI": ["10.1109/TIT.2006.871582"]}}, "BIBREF8": {"ref_id": "b8", "title": "The detection of defective members of large populations", "authors": [{"first": "Robert", "middle": [], "last": "Dorfman", "suffix": ""}], "year": 1943, "venue": "The Annals of Mathematical Statistics", "volume": "14", "issn": "4", "pages": "436--440", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Sparse nonnegative solution of underdetermined linear equations by linear programming", "authors": [{"first": "L", "middle": [], "last": "David", "suffix": ""}, {"first": "Jared", "middle": [], "last": "Donoho", "suffix": ""}, {"first": "", "middle": [], "last": "Tanner", "suffix": ""}], "year": 2005, "venue": "Proceedings of the National Academy of Sciences of the United States of America", "volume": "102", "issn": "", "pages": "9446--9451", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Counting the faces of randomly-projected hypercubes and orthants, with applications. Discrete and Computational Geometry", "authors": [{"first": "L", "middle": [], "last": "David", "suffix": ""}, {"first": "Jared", "middle": [], "last": "Donoho", "suffix": ""}, {"first": "", "middle": [], "last": "Tanner", "suffix": ""}], "year": 2010, "venue": "", "volume": "43", "issn": "", "pages": "522--541", "other_ids": {"DOI": ["10.1007/s00454-009-9221-z"]}}, "BIBREF11": {"ref_id": "b11", "title": "A Mathematical Introduction to Compressive Sensing. Birkh\u00e4user Basel", "authors": [{"first": "Simon", "middle": [], "last": "Foucart", "suffix": ""}, {"first": "Holger", "middle": [], "last": "Rauhut", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/978-0-8176-4948-7"]}}, "BIBREF12": {"ref_id": "b12", "title": "CVX: Matlab software for disciplined convex programming", "authors": [{"first": "Michael", "middle": [], "last": "Grant", "suffix": ""}, {"first": "Stephen", "middle": [], "last": "Boyd", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Statistical Learning with Sparsity: The Lasso and Generalizations", "authors": [{"first": "Trevor", "middle": [], "last": "Hastie", "suffix": ""}, {"first": "Robert", "middle": [], "last": "Tibshirani", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Wainwright", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Robust recovery of sparse nonnegative weights from mixtures of positivesemidefinite matrices", "authors": [{"first": "Fabian", "middle": [], "last": "Jaensch", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Jung", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.12005"]}}, "BIBREF15": {"ref_id": "b15", "title": "Understanding and Enhancing Data Recovery Algorithms. Dissertation, Technische Universitt Mnchen, Mnchen", "authors": [{"first": "Christian", "middle": [], "last": "Kmmerle", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Robust nonnegative sparse recovery and the nullspace property of 0/1 measurements", "authors": [{"first": "R", "middle": [], "last": "Kueng", "suffix": ""}, {"first": "P", "middle": [], "last": "Jung", "suffix": ""}], "year": 2018, "venue": "IEEE Transactions on Information Theory", "volume": "64", "issn": "2", "pages": "689--703", "other_ids": {"arXiv": ["arXiv:1603.07997"]}}, "BIBREF17": {"ref_id": "b17", "title": "Stable low-rank matrix recovery via null space properties", "authors": [{"first": "Maryia", "middle": [], "last": "Kabanava", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Kueng", "suffix": ""}, {"first": "Holger", "middle": [], "last": "Rauhut", "suffix": ""}, {"first": "Ulrich", "middle": [], "last": "Terstiege", "suffix": ""}], "year": null, "venue": "Journal of the IMA", "volume": "5", "issn": "4", "pages": "", "other_ids": {"DOI": ["10.1093/imaiai/iaw014"]}}, "BIBREF18": {"ref_id": "b18", "title": "Super-resolution of positive sources: The discrete setup", "authors": [{"first": "I", "middle": [], "last": "Veniamin", "suffix": ""}, {"first": "Emmanuel", "middle": ["J"], "last": "Morgenshtern", "suffix": ""}, {"first": "", "middle": [], "last": "Cands", "suffix": ""}], "year": 2016, "venue": "SIAM Journal on Imaging Sciences", "volume": "9", "issn": "1", "pages": "412--444", "other_ids": {"DOI": ["10.1137/15M1016552"]}}, "BIBREF19": {"ref_id": "b19", "title": "Graph implementations for nonsmooth convex programs", "authors": [{"first": "M", "middle": ["C"], "last": "Grant", "suffix": ""}, {"first": "S", "middle": ["P"], "last": "Boyd", "suffix": ""}], "year": 2008, "venue": "Recent Advances in Learning and Control", "volume": "371", "issn": "", "pages": "95--110", "other_ids": {"DOI": ["10.1007/978-1-84800-155-8_7"]}}, "BIBREF20": {"ref_id": "b20", "title": "Introductory Lectures on Convex Optimization -A Basic Course", "authors": [{"first": "Yurii", "middle": ["E"], "last": "Nesterov", "suffix": ""}], "year": 2004, "venue": "", "volume": "87", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1007/978-1-4419-8853-9"]}}, "BIBREF21": {"ref_id": "b21", "title": "Robust instance-optimal recovery of sparse signals at unknown noise levels", "authors": [{"first": "Bernd", "middle": [], "last": "Hendrik", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Petersen", "suffix": ""}, {"first": "", "middle": [], "last": "Jung", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Introduction to optimization. Translations series in mathematics and engineering", "authors": [{"first": "Boris", "middle": ["T"], "last": "Polyak", "suffix": ""}], "year": 1987, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Convex analysis. Princeton Mathematical Series", "authors": [{"first": "R", "middle": [], "last": "Tyrrell Rockafellar", "suffix": ""}], "year": 1970, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Sparse recovery by thresholded non-negative least squares", "authors": [{"first": "Martin", "middle": [], "last": "Slawski", "suffix": ""}, {"first": "Matthias", "middle": [], "last": "Hein", "suffix": ""}], "year": 2011, "venue": "", "volume": "", "issn": "", "pages": "1926--1934", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization", "authors": [{"first": "Martin", "middle": [], "last": "Slawski", "suffix": ""}, {"first": "Matthias", "middle": [], "last": "Hein", "suffix": ""}], "year": 2013, "venue": "Electron. J. Statist", "volume": "7", "issn": "", "pages": "3004--3056", "other_ids": {"DOI": ["10.1214/13-EJS868"]}}, "BIBREF26": {"ref_id": "b26", "title": "Sparse non-negative recovery from biased subgaussian measurements using NNLS. CoRR", "authors": [{"first": "Yonatan", "middle": [], "last": "Shadmi", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Jung", "suffix": ""}, {"first": "Giuseppe", "middle": [], "last": "Caire", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1901.05727"]}}, "BIBREF28": {"ref_id": "b28", "title": "Probing the pareto frontier for basis pursuit solutions", "authors": [{"first": "", "middle": [], "last": "Ewout Van Den", "suffix": ""}, {"first": "Michael", "middle": ["P"], "last": "Berg", "suffix": ""}, {"first": "", "middle": [], "last": "Friedlander", "suffix": ""}], "year": 2009, "venue": "SIAM Journal on Scientific Computing", "volume": "31", "issn": "2", "pages": "890--912", "other_ids": {"DOI": ["10.1137/080714488"]}}, "BIBREF29": {"ref_id": "b29", "title": "SPGL1: A solver for large-scale sparse reconstruction", "authors": [{"first": "E", "middle": [], "last": "Van Den", "suffix": ""}, {"first": "M", "middle": ["P"], "last": "Berg", "suffix": ""}, {"first": "", "middle": [], "last": "Friedlander", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "A unique \"nonnegative\" solution to an underdetermined system: From vectors to matrices", "authors": [{"first": "Meng", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Weiyu", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Ao", "middle": [], "last": "Tang", "suffix": ""}], "year": 2011, "venue": "IEEE Transactions on Signal Processing", "volume": "59", "issn": "3", "pages": "1007--1016", "other_ids": {"DOI": ["10.1109/TSP.2010.2089624"]}}}, "ref_entries": {"FIGREF0": {"text": "Let A \u2208 {0, 1} M\u00d7N and D \u2208 [M ]. For T \u2282 N the set Row (T ) := n\u2208T {m \u2208 [M ] such that A m,n = 1} is called the set of right vertices connected to the set of left vertices T . If # (Row ({n})) = D for all n \u2208 [N ] , then D \u22121 A \u2208 0, D \u22121 M\u00d7N is called a random walk matrix of a D-left regular bipartite graph. We also in short say that D \u22121 A is a D-LRBG. If additionally there exists a \u03b8 \u2208 [0, 1) such that # (Row (T )) \u2265 (1 \u2212 \u03b8) D |T | for all # (T ) \u2264 S holds true, then D \u22121 A is called a random walk matrix of a (S, D, \u03b8)-lossless expander.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "uniformly at random drawn D-LRBG is a random walk matrix of a (2S, D, \u03b8)-lossless expander with a high probability [FR13, Theorem 13.7]. Thus, recovery with the NNLAD is possible in the optimal regime M \u2208 O S log N S .", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Let A be D-LRBG. Each iteration of Algorithm 4.1 requires at most 4DN + 8N + 16M floating point operations and 5N + 4M assignments.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "1 with a high probability. If e is uniformly distributed on S M\u22121 0", "latex": null, "type": "figure"}, "FIGREF4": {"text": "almost the same performance as CLR/BPDN. EIHT fails for moderate S.", "latex": null, "type": "figure"}, "FIGREF5": {"text": ") NNLAD and NNLS perform roughly the same.", "latex": null, "type": "figure"}, "FIGREF6": {"text": "Performance of NNLAD for noise with even mass noise and varying sparsity of the signal.", "latex": null, "type": "figure"}, "FIGREF7": {"text": "The NNLS and NNLAD differ strongly.", "latex": null, "type": "figure"}, "FIGREF8": {"text": "Performance of NNLAD for noise with peaky mass and varying sparsity of the signal.", "latex": null, "type": "figure"}, "FIGREF9": {"text": "bounded by some constant. To verify this, we fix the constants r = 1, N = 1024, M = 256, D = 10, S = 32 and vary the signal to noise ratio SN R \u2208 10 [100]. For each SN R we repeat Experiment 1 100 times. We plot the mean of the logarithmic relative \u2113 1 -estimation error and the mean of the ratio of relative \u2113 1 -estimation error and \u2113 1 -noise power, i.e.Mean (LN\u2113 1 E) = Mean 10 log 10x \u2212 x sparsity. The result can be found inFigure 3aandFigure 3b.", "latex": null, "type": "figure"}, "FIGREF10": {"text": "The estimation error scales linearly with the noise power.", "latex": null, "type": "figure"}, "FIGREF11": {"text": "Performance of NNLAD for noise with even mass and varying noise power.", "latex": null, "type": "figure"}, "FIGREF12": {"text": "The estimation error does not scale linearly with the noise power.", "latex": null, "type": "figure"}, "FIGREF13": {"text": "Performance of NNLAD for noise with peaky mass and varying noise power.", "latex": null, "type": "figure"}, "FIGREF14": {"text": "If r = 1, generate a measurement matrix A \u2208 0, D \u22121 M\u00d7N as a uniformly at random drawn D-LRBG. If r = 2, draw each component A m,n of the measurement matrix independent and uniformly at random from {0, 1}, i.e. as 0/1 Bernoulli random variables. 2. Generate a signal x uniformly at random from \u03a3 S \u2229 R N + \u2229 S N \u22121 r . 3. Define the observation y := Ax. 4. For each iterative method calculate the sequence of estimators x k for all k \u2264 20000 and collect the relative estimation errors the time to calculate the first k iterations.", "latex": null, "type": "figure"}, "FIGREF15": {"text": "Convergence rates of certain iterated methods with respect to the number of iterations.", "latex": null, "type": "figure"}, "FIGREF16": {"text": "Convergence rates of certain iterated methods with respect to the time.", "latex": null, "type": "figure"}, "FIGREF17": {"text": "Time required to perform iterations of certain iterated methods.", "latex": null, "type": "figure"}, "FIGREF18": {"text": "Convergence rates of certain iterated methods with respect to the number of iterations. A is Bernoulli for NNLS and D-LRBG for the others.", "latex": null, "type": "figure"}, "FIGREF19": {"text": "Convergence rates of certain iterated methods with respect to the time. A is Bernoulli for NNLS and D-LRBG for the others.", "latex": null, "type": "figure"}, "FIGREF20": {"text": "The NNLAD is faster than the LASSO of SPGL1 for moderate S.", "latex": null, "type": "figure"}, "FIGREF21": {"text": "Time of the NNLAD and NNLS to approximate better than SPGL methods.", "latex": null, "type": "figure"}, "FIGREF22": {"text": "N persons, labeled by [N ] = {1, . . . , N }, to check whether or not they are affected by a virus. We denote by x n + e spe n the quantity of viruses in the specimen of the n-th person, where x n is the amount of viruses originating from the test person and e spe n is a the quantity of viruses originating from a possible contamination of the specimen. Suppose we have M test kits, labeled by [M ] = {1, . . . , M }. By y m we denote the amount of viruses in the sample of the m-th test kit. Let A \u2208 [0, 1] M\u00d7N . For every n we put a fraction of size A m,n of the specimen of the n-th person into the sample for the m-th test kit. The sample of the m-th test kit will then have the quantity of viruses y m = n\u2208[N ]", "latex": null, "type": "figure"}, "FIGREF23": {"text": "A m,n (x n + e spe n ) + e pro m ,", "latex": null, "type": "figure"}, "FIGREF25": {"text": "Relation of Saddle point and NNLAD ). Let y \u2208 R M as well as F (w) := w \u2212 y 1 and G (x) := 0 if x \u2265 0 \u221e else and f (x, w) := Ax, w + G (x) \u2212 F * (w) .", "latex": null, "type": "figure"}, "FIGREF26": {"text": "Let \u03c4, \u03c3 > 0, y \u2208 R M as well asF (w) := w \u2212 y 1 and G (x) |w m \u2212 y m | \u2264 \u03c3 w m \u2212 \u03c3 if w m \u2212 y m > \u03c3 w m + \u03c3 if w m \u2212 y m < Prox \u03c4 G (x) = P R N + (x)and Prox \u03c3F * (w) = (min {1, |w m \u2212 \u03c3y m |} sgn (w m \u2212 \u03c3y m )) m\u2208[M]  .", "latex": null, "type": "figure"}, "FIGREF27": {"text": "{1, |w m \u2212 \u03c3y m |} sgn (w m \u2212 \u03c3y m )) m\u2208[M] . Using Moreau's identity [Roc70, Theorem 31.5] yieldsProx \u03c3F * (w) =w \u2212 Prox (\u03c3F * ) * (w) = w \u2212 \u03c3Prox \u03c3 \u22121 F \u03c3 \u22121 w = (min {1, |w m \u2212 \u03c3y m |} sgn (w m \u2212 \u03c3y m )) m\u2208[M] ,which finishes the proof.After proving these auxiliary statements it remains to prove the main result of Section 4 about the convergence to a minimizer of NNLAD.Proof of Proposition 4.2. We setF (w) := w \u2212 y 1 and G (x) := 0 if x \u2265 0 \u221e else and f (x, w) := Ax, w + G (x) \u2212 F * (w) .", "latex": null, "type": "figure"}, "TABREF0": {"text": "Table of advantages of NNLAD and NNLS over each other.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>D-LRBG (\u21131) </td><td>Measurement Matrix biased sub-Gaussian (\u21132)\n</td></tr><tr><td>\u00a0</td><td>peaky \u2016e\u20161 </td><td>\u2248 \u2016e\u20162 </td><td>NNLAD </td><td>-\n</td></tr><tr><td>Noise </td><td>even mass \u2016e\u20161 unknown noise </td><td>\u2248M\n1\n2 \u2016e\u20162 </td><td>- </td><td>NNLS\n</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>\u00a0</td><td>NNLAD </td><td>NNLS\n</td></tr></table></body></html>"}, "TABREF2": {"text": "Roc70, Theorem 31.5]. For more information about saddle point problems, the fenchel conjugate and proximal point operators we refer the reader to[Roc70]. We have now the necessary means to state [CP11, Theorem 1].Theorem 8.1 ( [CP11, Theorem 1] ). Let F : R M \u2192 [0, \u221e) be convex and lower semicontinuous. Let G : R N \u2192 [0, \u221e] and F * : R M \u2192 [0, \u221e) 4 be proper, convex and lower semicontinous functions and A \u2208 R M\u00d7N .", "latex": null, "type": "table"}, "TABREF3": {"text": "Further, we need to calculate the iterates for this choice of F and G and thus the proximal point operators. It is well known that the proximal point operator of the \u2113 1 -norm is the soft thresholding operator. Using Moreau's identity[Roc70, Theorem 31.5] one can find the desired iterates directly. See for instance[FR13, Example 15.7].", "latex": null, "type": "table"}}, "back_matter": [{"text": "The work was partially supported by DAAD grant 57417688. PJ has been supported by DFG grant JU 2795/3. BB has been supported by BMBF through the German Research Chair at AIMS, administered by the Humboldt Foundation.", "cite_spans": [], "ref_spans": [], "section": "Acknowledgments"}, {"text": "Proof. The proximal point operator of an indicator function of a closed, convex set is always the projection to the set, hence the identity for G follows. For F this is more difficult. Note that w \u2032 is a minimizer ofif and only if zero is in the subdifferential at w \u2032 , which is given by the setSince the minimizer for the proximal operator is always unique, it remains to verify that zero is in the subdifferential at the vector from the statement. So let w \u2032 be the vector from the right hand side of (8) and m \u2208 [M ]. If w m \u2212 y m > \u03c3, then w \u2032 m = w m \u2212 \u03c3 > y m and thusIf w m \u2212 y m < \u2212\u03c3, then w \u2032 m = w m + \u03c3 < y m and thusIf |w m \u2212 y m | \u2264 \u03c3, we have w \u2032 m = y m and", "cite_spans": [], "ref_spans": [], "section": "annex"}]}