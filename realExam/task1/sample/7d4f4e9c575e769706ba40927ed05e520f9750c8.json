{"paper_id": "7d4f4e9c575e769706ba40927ed05e520f9750c8", "metadata": {"title": "Deep Multimodal Clustering with Cross Reconstruction", "authors": [{"first": "Xianchao", "middle": [], "last": "Zhang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Dalian University of Technology", "location": {"postCode": "116620", "settlement": "Dalian", "country": "China"}}, "email": ""}, {"first": "Xiaorui", "middle": [], "last": "Tang", "suffix": "", "affiliation": {"laboratory": "", "institution": "Dalian University of Technology", "location": {"postCode": "116620", "settlement": "Dalian", "country": "China"}}, "email": ""}, {"first": "Linlin", "middle": [], "last": "Zong", "suffix": "", "affiliation": {"laboratory": "", "institution": "Dalian University of Technology", "location": {"postCode": "116620", "settlement": "Dalian", "country": "China"}}, "email": "llzong@dlut.edu.cn"}, {"first": "Xinyue", "middle": [], "last": "Liu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Dalian University of Technology", "location": {"postCode": "116620", "settlement": "Dalian", "country": "China"}}, "email": ""}, {"first": "Jie", "middle": [], "last": "Mu", "suffix": "", "affiliation": {"laboratory": "", "institution": "Dalian University of Technology", "location": {"postCode": "116620", "settlement": "Dalian", "country": "China"}}, "email": ""}]}, "abstract": [{"text": "Recently, there has been surging interests in multimodal clustering. And extracting common features plays a critical role in these methods. However, since the ignorance of the fact that data in different modalities shares similar distributions in feature space, most works did not mining the inter-modal distribution relationships completely, which eventually leads to unacceptable common features. To address this issue, we propose the deep multimodal clustering with cross reconstruction method, which firstly focuses on multimodal feature extraction in an unsupervised way and then clusters these extracted features. The proposed cross reconstruction aims to build latent connections among different modalities, which effectively reduces the distribution differences in feature space. The theoretical analysis shows that the cross reconstruction reduces the Wasserstein distance of multimodal feature distributions. Experimental results on six benchmark datasets demonstrate that our method achieves obviously improvement over several state-of-arts.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Clustering is a vital research topic in data science and machine learning. Multimodal clustering is an important field in clustering and has made great progress. Multimodal clustering aims to divide the multimodal data information into different clusters in an unsupervised manner. The existing works usually are based on spectral clustering [14] , subspace clustering [1] , deep clustering [23] etc. On account of the amazing performance for feature extraction and dimensionality reduction tasks, deep clustering receives much attention in recent years [12, 17, 20, 23] . For deep multimodal clustering, the most common method extracts common features from different modalities [19] by using multiple deep neural networks (DNN) and clusters on the common features. Two autoencoders extract features from two modalities, VIB is used both in E1 and E2. x 1 are reconstructed from z 1 ,x 2 are reconstructed from z 2 . The dotted lines illustrate the cross reconstruction method,x 1 are reconstructed from z 2 ,x 2 are reconstructed from z 1 . The right part illustrates the second stage. The fusion layers fuse the features z 1 and z 2 to common features z * , which are used for clustering.", "cite_spans": [{"start": 342, "end": 346, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 369, "end": 372, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 391, "end": 395, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 554, "end": 558, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 559, "end": 562, "text": "17,", "ref_id": "BIBREF16"}, {"start": 563, "end": 566, "text": "20,", "ref_id": "BIBREF19"}, {"start": 567, "end": 570, "text": "23]", "ref_id": "BIBREF22"}, {"start": 679, "end": 683, "text": "[19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Introduction"}, {"text": "So far, the deep neural networks (DNN) based multimodal clustering methods can be divided into three categories: autoencoder based methods [16] , Deep Bolzmann Machine (DBM) based methods [19] and deep canonical correlation analysis (DCCA) [3] based methods [21] . The autoencoder based methods use autoencoders to extract common features of different modalities and choose common features that can best reconstruct the input data [16, 21] . However, autoencoders do not really discover the similarity of the common feature distributions. The DBM based methods [19] learn a joint representation of different modalities by DBM. But due to the high computational costs in high-dimensional data space [11] , the DBM based methods have not been widely studied in recent years. The DCCA based methods [21] learn features that are most correlated from different modalities by canonical correlation analysis (CCA). Like autoencoder based methods, DCCA based methods use autoencoders to extract features from different modalities, called deep canonically correlated autoencoder (DCCAE) [21] . The difference is that DCCAE further optimizes the canonical correlation among features of different modalities. But DCCA based methods lack the analysis of probability theory, which makes it difficult to measure the distribution differences of different modalities. Moreover, data of different modalities contain different numerical characteristics, and may not show obvious correlation. In this case, the depth typical correlation analysis may not be effective.", "cite_spans": [{"start": 139, "end": 143, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 188, "end": 192, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 240, "end": 243, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 258, "end": 262, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 431, "end": 435, "text": "[16,", "ref_id": "BIBREF15"}, {"start": 436, "end": 439, "text": "21]", "ref_id": "BIBREF20"}, {"start": 561, "end": 565, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 698, "end": 702, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 796, "end": 800, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 1078, "end": 1082, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper, we focus on multimodal clustering by extracting the common features of multimodal data in an unsupervised way, and reduce the distribution differences of different modalities in feature space. Firstly, we apply Variational Information Bottleneck (VIB) [2] to extract features from different modalities. By minimizing mutual information between raw data and the extracted features, VIB can control the amount of information owing through the network when extracting features. And due to the mutual information, VIB also provides explicit probabilistic analysis on feature space. Secondly, we apply cross recon-struction method during extracting features from different modalities, which can effectively reduce the distribution differences of different modalities in feature space. We also provide theoretical analysis to prove the similarity of multimodal distributions. Thirdly, we fuse the extracted features to common features using fusion layers. Finally, we cluster the common features using clustering algorithm. The entire process above constitute our deep multimodal clustering with cross reconstruction (DMCR).", "cite_spans": [{"start": 267, "end": 270, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Introduction"}, {"text": "The contributions of this work are summarized as below: (1) We propose a novel deep multimodal clustering algorithm, which can effectively reduce the distribution differences among different modalities in feature space. (2) We provide a theoretical analysis to prove that the proposed cross reconstruction method effectively reduce the distribution difference of different modalities in feature space. (3) Experiments show obviously improvement over state-of-the-art multimodal clustering methods on six benchmark multimodal datasets.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The existing deep clustering methods are roughly divided into two categories: two-stage methods and end-to-end methods [21] .", "cite_spans": [{"start": 119, "end": 123, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Deep Clustering"}, {"text": "The two-stage methods first extract features of the data by deep learning method, and finally apply the clustering methods to the features. Tian et al. [20] uses autoencoder to extract the features of graph and finally uses k-means to cluster. Chen [7] applies Deep Belief Network (DBN) to extract features and finally uses non-parametric maximum-margin clustering to cluster the features.", "cite_spans": [{"start": 152, "end": 156, "text": "[20]", "ref_id": "BIBREF19"}, {"start": 249, "end": 252, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Deep Clustering"}, {"text": "The end-to-end methods jointly optimize the feature extraction and clustering. The joint unsupervised learning (JULE) algorithm [24] uses a recurrent framework for joint unsupervised learning of deep representations and image clustering, which are optimized jointly in training process. The deep embedding clustering (DEC) algorithm [23] clusters a set of data points in a jointly optimized feature space. Based on DEC, the improved deep embedding clustering (IDEC) algorithm [12] jointly optimization and preserve local structure of data generating distribution.", "cite_spans": [{"start": 128, "end": 132, "text": "[24]", "ref_id": "BIBREF23"}, {"start": 333, "end": 337, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 476, "end": 480, "text": "[12]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Deep Clustering"}, {"text": "Based on the basic algorithms used in the multimodal methods, the existing multimodal clustering are roughly divided into two categories: the traditional clustering based methods [5, 6, 10, 22, 25] and deep clustering based methods [3, 16, 19, 21] .", "cite_spans": [{"start": 179, "end": 182, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 183, "end": 185, "text": "6,", "ref_id": "BIBREF5"}, {"start": 186, "end": 189, "text": "10,", "ref_id": "BIBREF9"}, {"start": 190, "end": 193, "text": "22,", "ref_id": "BIBREF21"}, {"start": 194, "end": 197, "text": "25]", "ref_id": "BIBREF24"}, {"start": 232, "end": 235, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 236, "end": 239, "text": "16,", "ref_id": "BIBREF15"}, {"start": 240, "end": 243, "text": "19,", "ref_id": "BIBREF18"}, {"start": 244, "end": 247, "text": "21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Multimodal Clustering"}, {"text": "The traditional clustering based methods learn a consensus matrix or minimize the divergence of multiple views simultaneously. For example, the multiview spectral clustering (MMSC) algorithm [5] learns a commonly shared graph Laplacian matrix by unifying different modalities. Gao et al. [10] proposes a novel NMF-based multi-view clustering algorithm by searching for a factorization that gives compatible clustering solutions across multimodal. The diversityinduced multi-view subspace clustering (DIMSC) algorithm [6] extend the existing subspace clustering into the multimodal domain. The low-rank tensor constrained multi-view subspace clustering (LT-MSC) algorithm [25] introduces a low-rank tensor constraint to explore the complementary information from multimodal data. The exclusivity-consistency regularized multi-view subspace clustering (ECMSC) algorithm [22] attempts to harness the complementary information between different representations by introducing a novel position-aware exclusivity term.", "cite_spans": [{"start": 191, "end": 194, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 288, "end": 292, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 517, "end": 520, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 671, "end": 675, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 868, "end": 872, "text": "[22]", "ref_id": "BIBREF21"}], "ref_spans": [], "section": "Multimodal Clustering"}, {"text": "Deep clustering based methods firstly jointly learn low-dimensional features from multimodal data, and then cluster the features. Ngiam et al. [16] proposes a series of frameworks for deep multimodal learning based on autoencoders. Srivastava and Salakhutdinov [19] proposes a deep multimodal representation learning framework, which learns a joint representation of different modalities by DBM. The DCCA [3] learns complex nonlinear transformations of two modalities of data such that the resulting representations are highly linearly correlated. The DCCAE [21] add an autoencoder regularization term to DCCA.", "cite_spans": [{"start": 143, "end": 147, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 261, "end": 265, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 405, "end": 408, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 558, "end": 562, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Multimodal Clustering"}, {"text": "In this section, we introduce our DMCR algorithm in detail. Consider the problem of clustering a set of n points", "cite_spans": [], "ref_spans": [], "section": "The Proposed Algorithm"}, {"text": ". Data in the same modality have the same dimensions. Multimodal clustering patitions data in m modalities into k clusters. Figure 1 illustrates the framework of DMCR for two modalities. The DMCR has two stages. In the first stage, we extract features from different modalities using VIB and cross reconstruction to constrain encoders, which ensures the extracted features of different modalities sharing similar distributions. In the first stage of Fig. 1 , E 1 and D 1 are the encoder and decoder used for the first modality, E 2 and D 2 are the encoder and decoder used for the second modality; z 1 and z 2 are features extracted from x 1 and x 2 ; VIB in E 1 and E 2 denote the VIB regularization terms;x 1 andx 2 are data reconstructed from z 1 and z 2 ;x 1 andx 2 are data reconstructed from cross reconstruction; In the second stage, we fuse the features to common features by fusion layers, then we cluster these fused common features. In the second stage of Fig. 1 , the fusion layers fuse features from different modalities to common features, z * is the common features. We describe our algorithm next.", "cite_spans": [], "ref_spans": [{"start": 124, "end": 132, "text": "Figure 1", "ref_id": "FIGREF0"}, {"start": 450, "end": 456, "text": "Fig. 1", "ref_id": "FIGREF0"}, {"start": 967, "end": 973, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "The Proposed Algorithm"}, {"text": "Multimodal data contain modality-unique features and modality-common features. It is difficult to extract the modality-common features directly from different modalities with traditional encoders. We first use the Variational Information Bottleneck (VIB) [2] to extract the modality-common features among different modalities. And then apply Cross Reconstruction method to ensure the modality-common features of different modalities satisfy similar distribution.", "cite_spans": [{"start": 255, "end": 258, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "Multimodal Feature Extraction with VIB. We adopt deep autoencoders as a features extractor. For a given input x i in the i-th modality, the encoder aims to get a feature z i in a low-dimensional space, and the decoder aims to reconstruct the input from the latent representation. So, the goal of the deep autoencoder is to get a good reconstruction x i for input x i .", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "But autoencoders cannot control the amount of information contained in extracted features, which makes it difficult to extract modality-common features from each modality.", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "Given that VIB is able to control the scale of feature information, we use the VIB [2] regularization term in the encoders to eliminate unique features and extract the common features. The loss function of extracting features of the i-th modality is: (1) where p(z i |x i ; \u03b8 i ) denotes the encoder for the i-th modality; \u03b8 i is the parameter of the encoder; q(x i |z i ; \u03d5 i ) denotes the decoder for the i-th modality; z i are generated with reparameterization trick [2, 13] ; \u03d5 i is the parameter of the decoder; \u03b2 controls the weight of the VIB regularization term. The first term", "cite_spans": [{"start": 83, "end": 86, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 470, "end": 473, "text": "[2,", "ref_id": "BIBREF1"}, {"start": 474, "end": 477, "text": "13]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "in Eq. 1 is the reconstruction loss of autoencoder for the i-th modality. The second term in Eq. 1 is the VIB regularization loss for the i-th modality.", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "Cross Reconstruction. In the last section, we only extract modality-common features from each modality using VIB. In multimodal learning tasks, the basic task is mining the relationships among different modalities. Previous work, such as DCCAE [21] learn complex nonlinear transformations of two modalities of data such that the resulting representations are highly linearly correlated. But data of different modalities have different numerical characteristics, correlation constraints on different modalities may failed to capture the statistical properties of implicit features. Considering the fact that modality-common features of different modalities share similar distributions, we propose a cross reconstruction method to promote the features extraction process.", "cite_spans": [{"start": 244, "end": 248, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "The dotted lines in Fig. 1 represent the our cross reconstruction process. The decoders D 1 and D 2 both take z 1 and z 2 as inputs simultaneously. Specifically, the green dotted lines in Fig. 1 represent reconstructing x 1 with z 2 using D 1 and the red dotted lines in Fig. 1 represent reconstructing x 2 with z 1 using D 2 . The loss function of reconstructing x j with z i (j = i) is:", "cite_spans": [], "ref_spans": [{"start": 20, "end": 26, "text": "Fig. 1", "ref_id": "FIGREF0"}, {"start": 188, "end": 194, "text": "Fig. 1", "ref_id": "FIGREF0"}, {"start": 271, "end": 277, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Multimodal Feature Extraction"}, {"text": "where q(x j |z i ; \u03d5 j ) denotes the decoder used for reconstructing x j with z i . Note that, it is also the same decoder for reconstructing x j with z j . As a regularization term, cross reconstruction restrains the similarity of different modalities in distribution, which will analysed later.", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "The Overall Loss Function. The complete loss function of extracting feature of the i-th modality is:", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "where j represents a modality except the i-th modality; the last term of Eq. 3 is cross reconstruction regularization loss of reconstructing x j with z i ; \u03b3 controls the weight of the cross reconstruction regularization term.", "cite_spans": [], "ref_spans": [], "section": "Multimodal Feature Extraction"}, {"text": "Reconstruction.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 1. Training Process of Deep Multimodal Clustering with Cross"}, {"text": "The number of modality m;", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "The dataset of n data points for each modality i:", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "1: for number of training iterations do 2:", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "Sampling a minibatch of samples from each modality;", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "3: 16 :", "cite_spans": [{"start": 3, "end": 5, "text": "16", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Input:"}, {"text": "17:", "cite_spans": [], "ref_spans": [], "section": "5:"}, {"text": "end for 19 :", "cite_spans": [{"start": 8, "end": 10, "text": "19", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "18:"}, {"text": "Training the fusion layer with Eq.5;", "cite_spans": [], "ref_spans": [], "section": "20:"}, {"text": "After extracting features from different modalities, we fuse these features to common features, then cluster on the common features. As shown in the second stage of Fig. 1 , we use fusion layers to fuse extracted features. The fusion layers consist of fully connected layers, and z * is the fused common features:", "cite_spans": [], "ref_spans": [{"start": 165, "end": 171, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Feature Fusion"}, {"text": "where \u03b7 is the parameter of the fusion layers. We use L2 loss between z * and extracted features of different modalities to train the fusion layers:", "cite_spans": [], "ref_spans": [], "section": "Feature Fusion"}, {"text": "In this section, we describe the training process and clustering process of DMCR. The entire training process has two steps: (1) Training the autoencoders with Eq. 3. (2) Training the fusion layers according to Eq. 5. The training process of DMCR is summarized in Algorithm 1.", "cite_spans": [], "ref_spans": [], "section": "The DMCR Algorithm"}, {"text": "After training DMCR, we get the common features associated with multimodal data. Then we cluster the common features. Here, we choose k-means as the final clustering algorithm. The clustering process of DMCR is summarized in Algorithm 2.", "cite_spans": [], "ref_spans": [], "section": "The DMCR Algorithm"}, {"text": "Reconstruction.", "cite_spans": [], "ref_spans": [], "section": "Algorithm 2. Clustering Process of Deep Multimodal Clustering with Cross"}, {"text": "The number of modality m;", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "The dataset of n data points for each modality i:", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "The number of clusters k.", "cite_spans": [], "ref_spans": [], "section": "Input:"}, {"text": "Cluster assignments for n multimodal data points:", "cite_spans": [], "ref_spans": [], "section": "Output:"}, {"text": "z i = Reparameterization(\u03bc(x i ), \u03c3(x i )); 4: end for 5: z * = Fusion(z 1 , z 2 , ..., z m ; \u03b7); 6: C = k-means(z * , k).", "cite_spans": [], "ref_spans": [], "section": "3:"}, {"text": "As mentioned previously, cross reconstruction can build implicit connection among different modalities. In this section, we explore the probability theory of connection among different modalities built by cross reconstruction method.", "cite_spans": [], "ref_spans": [], "section": "Theoretical Analysis"}, {"text": "Here we assume q(x i |z i ; \u03d5 i ) to be a Gaussian distribution in the i-th modality with mean \u03bc i (z i ) and variance \u03c3 2 i (z i ), q(x i |z j ; \u03d5 i ) to be a Gaussian distribution in the i-th modality with mean \u03bc j (z j ) and variance \u03c3 2 j (z j ). And then we can further derive the loss function of cross reconstruction:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Analysis"}, {"text": "where L i represents the loss of reconstructing x i with z i ; L j represents the loss of reconstructing x i with z j . Combining L i and L j , both \u03bc i (z i ) and \u03bc j (z j ) reduce the difference from x i , which means that the difference between \u03bc i (z i ) and \u03bc j (z j ) also decrease. Note that both \u03bc i (z i ) and \u03bc j (z j ) are outputs of decoder D i , so the input of D i , i.e. z i and z j are similar. z i and z j are generated with reparameterization trick:", "cite_spans": [], "ref_spans": [], "section": "Theoretical Analysis"}, {"text": "where \u03bc(x i ) and \u03c3(x i ) are the outputs of E i , \u03bc(x j ) and \u03c3(x j ) are the outputs of E j ; z i and z j are randomly sampled from Gaussian distribution N i (\u03bc(x i ), \u03c3(x i )) and N j (\u03bc(x j ), \u03c3(x j )) respectively. So the Wasserstein distance [4] between N i and N j also decrease:", "cite_spans": [{"start": 248, "end": 251, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "Theoretical Analysis"}, {"text": "where (N i , N j ) denotes the set of all joint distributions where the marginals of (z i , z j ) are N i and N j respectively. Therefore, under the constraints of cross reconstruction, these encoders will reduce the distribution differences of multimodal features. So we prove that the cross reconstruction constrain extracted features to share similar distributions in feature spaces of different modalities.", "cite_spans": [], "ref_spans": [], "section": "Theoretical Analysis"}, {"text": "We test our model on six multimodal datasets: Digits 1 , CNN 2 , AwA 3 , Cal101 4 , LUse-21 5 and Scene-15 [9] : Digits contains three modalities of 2000 samples belonging to 10 clusters. The three modalities respectively have 76, 216 and 240 dimensions. CNN is a news dataset that contains two modalities of 2107 samples belonging to 7 clusters. The first modality consists of text contents, the second modality contains the images of articles. AwA contains three modalities of 5814 samples belonging to 10 clusters. These three modalities are local self-similarity features, SIFT features and SURF features. Cal101, LUse-21 and Scene-15 contain three modalities: we extract LBP, GIST and CENTRIST descriptors from these datasets as three modalities. Cal101 contains 712 samples and these samples belong to 10 clusters. LUse-21 contains 2100 samples belonging to 21 clusters. Scene-15 contains 3000 samples assigned to 15 clusters. ", "cite_spans": [{"start": 92, "end": 93, "text": "5", "ref_id": "BIBREF4"}, {"start": 107, "end": 110, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Description of Datasets"}, {"text": "We compare the proposed DMCR algorithm with the following baselines: (1) Single modal clustering: DEC [23] , IDEC [12] and JULE [24] . We test these methods on each modality data and take the best result as their final result.", "cite_spans": [{"start": 102, "end": 106, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 114, "end": 118, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 128, "end": 132, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Comparing Methods"}, {"text": "Multimodal clustering: MMSC [5] , RMKMC [10] , DIMSC [6] , LT-MSC [25] , ECMSC [22] and DCCAE [21] . Among these multimodal clustering methods, DCCAE is a two-modal method. In order to extend DCCAE to multimodal clustering task, we combine every two of the multiple modalities, and take the average results as the final result. (3) The simplified DMCR: the DMCR without cross reconstruction regularization term, called DMC. ", "cite_spans": [{"start": 28, "end": 31, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 40, "end": 44, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 53, "end": 56, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 66, "end": 70, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 79, "end": 83, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 94, "end": 98, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Comparing Methods"}, {"text": "The model and parameter settings of our experiments are follows: (1) We keep these parameter settings of comparing methods as the original papers. During training, we fine-tune the parameters of these methods to get the best performance as the final result. (2) We use three autoencoders to handle three modalities and two autoencoders for two modalities. The encoders and decoders are composed of fully connected layers. We use sigmoid as the activation function in the last layer of decoders, and use ReLU activation in the other layers of encoders and decoders. The parameters of our method are randomly initialized and we set the learning rate of Adam to 0.001. (3) We note that the k-means, which is the final step of DMCR, can be replaced by other clustering algorithms. But considering the interpretation of the Euclidean distance in the feature space as diffusion distance in the input space [8, 15, 18] , we choose k-means as the final clustering algorithm.", "cite_spans": [{"start": 900, "end": 903, "text": "[8,", "ref_id": "BIBREF7"}, {"start": 904, "end": 907, "text": "15,", "ref_id": "BIBREF14"}, {"start": 908, "end": 911, "text": "18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Model and Parameter Settings"}, {"text": "Clustering Results. We evaluate our approach on three metrics of Accuracy (ACC), Normalized Mutual Information (NMI), and Purity. The experiment results of clustering ACC, NMI and Purity are summarized in Table 1, Table 2 and Table 3 . The best results are marked in bold. Firstly, we compare DMCR with the single modal algorithms DEC, IDEC and JULE. DMCR performs better than DEC and IDEC on each dataset in terms of ACC, NMI and Purity. DMCR outperforms JULE in most cases in terms of ACC, NMI and Purity. As exceptional case, DMCR performs worse than JULE on Cal101 dataset in terms of Purity. The Purity of JULE only increased by 4%, however, the NMI of JULE is decreased by 10%. Generally, it can be seen that DMCR outperforms the single modal algorithms when clustering multimodal data. The results indicate that it is reasonable to ensemble multimodal.", "cite_spans": [], "ref_spans": [{"start": 205, "end": 221, "text": "Table 1, Table 2", "ref_id": "TABREF1"}, {"start": 226, "end": 233, "text": "Table 3", "ref_id": null}], "section": "Experiment Results"}, {"text": "Secondly, we compare DMCR with multi-modal methods MMSC, RMKMC, DIMSC, LT-MSC, ECMSC and DCCAE. DMCR outperforms MMSC, RMKMC, DIMSC and ECMSC on each dataset in terms of ACC, NMI and Purity. DMCR outperforms LT-MSC and DCCAE in most cases. Taking NMI for example, the NMI of DMCR raises 12% on the Digits dataset, 3% on the AwA dataset, 13% on the Cal101 dataset, 10% on the LUse-21 dataset, 14% on the LUse-21 dataset. As exceptional cases, LT-MSC achieves a better Purity on Cal101 which is about 0.1% higher than DMCR, and DCCAE achieves a better Purity on Cal101 which is about 2% higher than DMCR. However, the NMI of LT-MSC is decreased by 10%, and the NMI of DCCAE is decreased by 7%. Generally, DMCR outperforms the other multimodal methods.", "cite_spans": [], "ref_spans": [], "section": "Experiment Results"}, {"text": "Finally, we compare DMCR with DMC, we can find that DMCR outperforms DMC, especially on Digits, CNN and Scene-15, which is more than 5% higher than DMC. That proves that the cross reconstruction regularization is effective for extracting common features of different modalities. From these tables, we can also observe that DMC also demonstrates strong competitiveness compared to other models. That proves that VIB is a effective feature extraction method that is universally applicable to different datasets. Note that DEC and IDEC just use an autoencoder without VIB, and do not perform well on multimodal datasets.", "cite_spans": [], "ref_spans": [], "section": "Experiment Results"}, {"text": "In summary, it can be concluded that our method performs the best on the multimodal datasets. The proposed cross reconstruction regularization improves the results of multimodal clustering, which further proves that it is beneficial to establish a connection among different modalities.", "cite_spans": [], "ref_spans": [], "section": "Experiment Results"}, {"text": "Parameter Setting Results. In Table 4 , we explore the effect of the parameters \u03b2 and \u03b3 to clustering performance of DMCR on each dataset. Due to space limitation, we only present the experimental results on the Digits dataset and Cal101 dataset in this paper. Both \u03b2 and \u03b3 vary in the set [0, 0.5, 1] and the best results are marked in bold. The value of \u03b2 and \u03b3 separately reflects how much we want to enforce the cross reconstruction regularization and the VIB regularization. The \u03b2 = 1, \u03b3 = 0 stands for DMCR without cross reconstruction regularization, the\u03b2 = 1, \u03b3 = 0.5 stands for DMCR without VIB regularization, and the \u03b2 = 1, \u03b3 = 0 stands for DMCR without both cross reconstruction regularization and VIB regularization. It can be seen that the performance of DMCR without one regularization is better than that of DMCR without both regularization, but is worse than that of DMCR with both regularization, which proves the validity of the VIB regularization and cross reconstruction regularization. Furthermore, as shown in the tables, we get the best results when \u03b2 = 1 and \u03b3 = 0.5.", "cite_spans": [], "ref_spans": [{"start": 30, "end": 37, "text": "Table 4", "ref_id": null}], "section": "Experiment Results"}, {"text": "In this paper, we propose a novel deep multimodal clustering framework called DMCR. Firstly, we control the scale of feature using VIB. Secondly, we reduce the distribution differences among multimodal features using cross reconstruction. Thirdly, we fuse the extracted features to common features. Finally, we cluster the common features using k-means. In addition, we prove that the proposed cross reconstruction method effectively reduce the distribution differences of multimodal features. We compare our DMCR algorithm with the state-ofthe-art multimodal methods on many multimodal datasets. Experimental results show that our algorithm achieves obviously improvement on multimodal clustering task.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Automatic subspace clustering of high dimensional data for data mining applications", "authors": [{"first": "R", "middle": [], "last": "Agrawal", "suffix": ""}, {"first": "J", "middle": [], "last": "Gehrke", "suffix": ""}, {"first": "D", "middle": [], "last": "Gunopulos", "suffix": ""}, {"first": "P", "middle": [], "last": "Raghavan", "suffix": ""}], "year": 1998, "venue": "SIGMOD Conference", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Deep variational information bottleneck", "authors": [{"first": "A", "middle": ["A"], "last": "Alemi", "suffix": ""}, {"first": "I", "middle": [], "last": "Fischer", "suffix": ""}, {"first": "J", "middle": ["V"], "last": "Dillon", "suffix": ""}, {"first": "K", "middle": [], "last": "Murphy", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Deep canonical correlation analysis", "authors": [{"first": "G", "middle": [], "last": "Andrew", "suffix": ""}, {"first": "R", "middle": [], "last": "Arora", "suffix": ""}, {"first": "J", "middle": ["A"], "last": "Bilmes", "suffix": ""}, {"first": "K", "middle": [], "last": "Livescu", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Heterogeneous image feature integration via multi-modal spectral clustering", "authors": [{"first": "X", "middle": [], "last": "Cai", "suffix": ""}, {"first": "F", "middle": [], "last": "Nie", "suffix": ""}, {"first": "H", "middle": [], "last": "Huang", "suffix": ""}, {"first": "F", "middle": [], "last": "Kamangar", "suffix": ""}], "year": 2011, "venue": "CVPR", "volume": "", "issn": "", "pages": "1977--1984", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Diversity-induced multi-view subspace clustering", "authors": [{"first": "X", "middle": [], "last": "Cao", "suffix": ""}, {"first": "C", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "H", "middle": [], "last": "Fu", "suffix": ""}, {"first": "S", "middle": [], "last": "Liu", "suffix": ""}, {"first": "H", "middle": [], "last": "Zhang", "suffix": ""}], "year": 2015, "venue": "CVPR", "volume": "", "issn": "", "pages": "586--594", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Deep learning with nonparametric clustering", "authors": [{"first": "G", "middle": [], "last": "Chen", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Diffusion maps", "authors": [{"first": "R", "middle": ["R"], "last": "Coifman", "suffix": ""}, {"first": "S", "middle": [], "last": "Lafon", "suffix": ""}], "year": 2006, "venue": "Appl. Comput. Harmonic Anal", "volume": "21", "issn": "", "pages": "5--30", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "A Bayesian hierarchical model for learning natural scene categories", "authors": [{"first": "L", "middle": [], "last": "Fei-Fei", "suffix": ""}, {"first": "P", "middle": [], "last": "Perona", "suffix": ""}], "year": 2005, "venue": "", "volume": "2", "issn": "", "pages": "524--531", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Multi-view clustering via joint nonnegative matrix factorization", "authors": [{"first": "J", "middle": [], "last": "Gao", "suffix": ""}, {"first": "J", "middle": [], "last": "Han", "suffix": ""}, {"first": "J", "middle": [], "last": "Liu", "suffix": ""}, {"first": "C", "middle": [], "last": "Wang", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "NIPS 2016 tutorial: Generative adversarial networks", "authors": [{"first": "I", "middle": ["J"], "last": "Goodfellow", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Improved deep embedded clustering with local structure preservation", "authors": [{"first": "X", "middle": [], "last": "Guo", "suffix": ""}, {"first": "L", "middle": [], "last": "Gao", "suffix": ""}, {"first": "X", "middle": [], "last": "Liu", "suffix": ""}, {"first": "J", "middle": [], "last": "Yin", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Auto-encoding variational Bayes", "authors": [{"first": "D", "middle": ["P"], "last": "Kingma", "suffix": ""}, {"first": "M", "middle": [], "last": "Welling", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "A tutorial on spectral clustering", "authors": [{"first": "U", "middle": [], "last": "Von Luxburg", "suffix": ""}], "year": 2007, "venue": "Stat. Comput", "volume": "17", "issn": "", "pages": "395--416", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators", "authors": [{"first": "B", "middle": [], "last": "Nadler", "suffix": ""}, {"first": "S", "middle": [], "last": "Lafon", "suffix": ""}, {"first": "R", "middle": ["R"], "last": "Coifman", "suffix": ""}, {"first": "I", "middle": ["G"], "last": "Kevrekidis", "suffix": ""}], "year": 2005, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Multimodal deep learning. In: ICML", "authors": [{"first": "J", "middle": [], "last": "Ngiam", "suffix": ""}, {"first": "A", "middle": [], "last": "Khosla", "suffix": ""}, {"first": "M", "middle": [], "last": "Kim", "suffix": ""}, {"first": "J", "middle": [], "last": "Nam", "suffix": ""}, {"first": "H", "middle": [], "last": "Lee", "suffix": ""}, {"first": "A", "middle": ["Y"], "last": "Ng", "suffix": ""}], "year": 2011, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Deep subspace clustering with sparsity prior", "authors": [{"first": "X", "middle": [], "last": "Peng", "suffix": ""}, {"first": "S", "middle": [], "last": "Xiao", "suffix": ""}, {"first": "J", "middle": [], "last": "Feng", "suffix": ""}, {"first": "W", "middle": ["Y"], "last": "Yau", "suffix": ""}, {"first": "Z", "middle": [], "last": "Yi", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "SpectralNet: spectral clustering using deep neural networks", "authors": [{"first": "U", "middle": [], "last": "Shaham", "suffix": ""}, {"first": "K", "middle": ["P"], "last": "Stanton", "suffix": ""}, {"first": "H", "middle": [], "last": "Li", "suffix": ""}, {"first": "B", "middle": [], "last": "Nadler", "suffix": ""}, {"first": "R", "middle": [], "last": "Basri", "suffix": ""}, {"first": "Y", "middle": [], "last": "Kluger", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Multimodal learning with deep Boltzmann machines", "authors": [{"first": "N", "middle": [], "last": "Srivastava", "suffix": ""}, {"first": "R", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": 2012, "venue": "J. Mach. Learn. Res", "volume": "15", "issn": "", "pages": "2949--2980", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Learning deep representations for graph clustering", "authors": [{"first": "F", "middle": [], "last": "Tian", "suffix": ""}, {"first": "B", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Q", "middle": [], "last": "Cui", "suffix": ""}, {"first": "E", "middle": [], "last": "Chen", "suffix": ""}, {"first": "T", "middle": ["Y"], "last": "Liu", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "On deep multi-view representation learning", "authors": [{"first": "W", "middle": [], "last": "Wang", "suffix": ""}, {"first": "R", "middle": [], "last": "Arora", "suffix": ""}, {"first": "K", "middle": [], "last": "Livescu", "suffix": ""}, {"first": "J", "middle": ["A"], "last": "Bilmes", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Exclusivity-consistency regularized multi-view subspace clustering", "authors": [{"first": "X", "middle": [], "last": "Wang", "suffix": ""}, {"first": "X", "middle": [], "last": "Guo", "suffix": ""}, {"first": "Z", "middle": [], "last": "Lei", "suffix": ""}, {"first": "C", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "S", "middle": ["Z"], "last": "Li", "suffix": ""}], "year": 2017, "venue": "CVPR", "volume": "", "issn": "", "pages": "1--9", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Unsupervised deep embedding for clustering analysis", "authors": [{"first": "J", "middle": [], "last": "Xie", "suffix": ""}, {"first": "R", "middle": ["B"], "last": "Girshick", "suffix": ""}, {"first": "A", "middle": [], "last": "Farhadi", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Joint unsupervised learning of deep representations and image clusters", "authors": [{"first": "J", "middle": [], "last": "Yang", "suffix": ""}, {"first": "D", "middle": [], "last": "Parikh", "suffix": ""}, {"first": "D", "middle": [], "last": "Batra", "suffix": ""}], "year": 2016, "venue": "CVPR", "volume": "", "issn": "", "pages": "5147--5156", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Low-rank tensor constrained multiview subspace clustering", "authors": [{"first": "C", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "H", "middle": [], "last": "Fu", "suffix": ""}, {"first": "S", "middle": [], "last": "Liu", "suffix": ""}, {"first": "G", "middle": [], "last": "Liu", "suffix": ""}, {"first": "X", "middle": [], "last": "Cao", "suffix": ""}], "year": 2015, "venue": "ICCV", "volume": "", "issn": "", "pages": "1582--1590", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "The two stages of DMCR framework: The left part illustrates the first stage.", "latex": null, "type": "figure"}, "TABREF1": {"text": "", "latex": null, "type": "table"}, "TABREF2": {"text": "Clustering NMI (%)", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>DEC </td><td>IDEC </td><td>JULE </td><td>DCCAE </td><td>MMSC </td><td>RMKMC </td><td>LT-MSC </td><td>DIMSC </td><td>ECMSC </td><td>DMC </td><td>DMCR\n</td></tr><tr><td>Digits </td><td>35.11 </td><td>44.46 </td><td>70.25 </td><td>67.59 </td><td>57.21 </td><td>70.74 </td><td>70.40 </td><td>39.25 </td><td>71.21 </td><td>74.98 </td><td>83.17\n</td></tr><tr><td>CNN </td><td>20.79 </td><td>13.28 </td><td>18.44 </td><td>14.27 </td><td>11.31 </td><td>13.07 </td><td>13.84 </td><td>12.67 </td><td>18.87 </td><td>37.74 </td><td>50.93\n</td></tr><tr><td>AwA </td><td>4.10 </td><td>6.73 </td><td>8.33 </td><td>10.13 </td><td>1.81 </td><td>9.23 </td><td>9.24 </td><td>7.61 </td><td>7.33 </td><td>10.20 </td><td>12.72\n</td></tr><tr><td>Cal101 </td><td>22.63 </td><td>31.00 </td><td>54.12 </td><td>57.96 </td><td>45.55 </td><td>50.49 </td><td>53.41 </td><td>33.74 </td><td>51.53 </td><td>57.51 </td><td>64.31\n</td></tr><tr><td>LUse-21 </td><td>15.18 </td><td>28.02 </td><td>34.99 </td><td>32.05 </td><td>26.56 </td><td>30.55 </td><td>35.62 </td><td>21.69 </td><td>25.93 </td><td>39.17 </td><td>40.39\n</td></tr><tr><td>Scene-15 </td><td>16.10 </td><td>19.92 </td><td>35.84 </td><td>37.82 </td><td>14.44 </td><td>39.03 </td><td>41.36 </td><td>19.39 </td><td>41.16 </td><td>40.58 </td><td>55.55\n</td></tr></table></body></html>"}, "TABREF3": {"text": "Clustering Purity (%) DEC IDEC JULE DCCAE MMSC RMKMC LT-MSC DIMSC ECMSC DMC DMCR Parameter Setting results on Digits and Cal101 (%) = 0.5, \u03b3 = 0.5 90.35 82.46 90.35 63.76 63.68 63.34 \u03b2 = 1, \u03b3 = 0.5 90.75 83.17 90.75 66.57 64.31 64.46 \u03b2 = 0, \u03b3 = 1 80.50 81.85 84.05 58.00 59.88 63.34 \u03b2 = 0.5, \u03b3 = 1 90.25 82.60 90.25 62.92 61.08 59.26 \u03b2 = 1, \u03b3 = 1 90.55 82.02 90.25 58.14 58.57 61.65", "latex": null, "type": "table"}}, "back_matter": []}