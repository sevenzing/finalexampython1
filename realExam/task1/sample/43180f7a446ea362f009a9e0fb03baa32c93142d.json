{"paper_id": "43180f7a446ea362f009a9e0fb03baa32c93142d", "metadata": {"title": "ISeeU2: Visually Interpretable ICU mortality prediction using deep learning and free-text medical notes", "authors": [{"first": "William", "middle": [], "last": "Caicedo-Torres", "suffix": "", "affiliation": {"laboratory": "", "institution": "Auckland University of Technology", "location": {}}, "email": ""}, {"first": "Jairo", "middle": [], "last": "Gutierrez", "suffix": "", "affiliation": {"laboratory": "", "institution": "Auckland University of Technology", "location": {}}, "email": ""}]}, "abstract": [{"text": "Accurate mortality prediction allows Intensive Care Units (ICUs) to adequately benchmark clinical practice and identify patients with unexpected outcomes. Traditionally, simple statistical models have been used to assess patient death risk, many times with sub-optimal performance. On the other * Corresponding author", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Intensive Care Units (ICUs) are the last line of defense against critical conditions that require constant monitoring and advanced medical support.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Their importance has been highlighted in recent times, when ICUs around the world have been overrun by the COVID-19 pandemic [1, 2] . It is in times like these when research into ways to adequately manage scarce critical care resources must be even more vigorously pursued, in order to offer additional tools that support medical decisions and allow for the effective benchmark of clinical practice.", "cite_spans": [{"start": 125, "end": 128, "text": "[1,", "ref_id": "BIBREF0"}, {"start": 129, "end": 131, "text": "2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Introduction"}, {"text": "The issue of mortality prediction in the ICU has been approached from a statistical standpoint by means of risk prediction models like APACHE, SAPS, MODS, among others [3] . These models use a set of physiological predictors, demographic factors, and the occurrence of certain chronic conditions, to estimate a score that serves as a proxy for the likelihood of death of ICU patients. Because of the relatively straightforward way of interpreting results, simple statistical approaches such as logistic regression are the go-to modeling techniques used to estimate mortality probability and the importance of the predictors involved. On the other hand, the simplicity of the models also mean that their limited expressiveness may not accurately represent the possibly non-linear dynamics of mortality prediction. Given this, high-capacity machine learning models might be useful to increase predictive performance. Concretely, the relevant literature shows that the use of deep learning models trained on physiological time-series data can outperform these previously mentioned statistical models [4, 5] .", "cite_spans": [{"start": 168, "end": 171, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 1097, "end": 1100, "text": "[4,", "ref_id": "BIBREF3"}, {"start": 1101, "end": 1103, "text": "5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Introduction"}, {"text": "One of the advantages of deep learning over other techniques is its ability to use multiple modes of data to train predictive models. In the biomedical domain, health records, images, and time-series data, have been used for different tasks with success [6, 7] . This advantage is relevant for mortality prediction (and for many other clinical tasks as well), as a substantial amount of data is generated inside ICUs as free-text notes which can be used as input to create Natural Language Processing (NLP) predictive models. The nature of NLP poses some challenges for which deep learning is uniquely suited via its ability to deal with high-dimensional data and its elegant way to take temporal and spatial patterns into account. Some works have used deep neural networks and free-text to predict mortality [8] and length of stay (among others), showing that there is interesting potential for this type of models.", "cite_spans": [{"start": 254, "end": 257, "text": "[6,", "ref_id": "BIBREF5"}, {"start": 258, "end": 260, "text": "7]", "ref_id": "BIBREF7"}, {"start": 809, "end": 812, "text": "[8]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Introduction"}, {"text": "On the other hand, a particularly important downside of deep learning is that, compared to the simpler logistic regression based models, feature importance is not as readily available. This in turn makes these models hard to interpret, as internally the model may transform the original input features to high-dimensional spaces via non-linear transformations, making it hard to establish the impact of each predictor on the predicted outcome. It has been documented that given their large predictive capacity, deep learning models can easily fit spurious correlations in the datasets used for their training, leading to potential diagnostic issues [9] . However some work has been done to interpret deep learning models in order to offer explanations intended to foster trust and further encourage their usage in the critical care setting. For instance, in our previous work we developed an interpretable deep learning mortality prediction model that uses physiological time-series data from the first 48 hours of patient ICU stay [5] .", "cite_spans": [{"start": 649, "end": 652, "text": "[9]", "ref_id": null}, {"start": 1032, "end": 1035, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Introduction"}, {"text": "In this work, we present ISeeU2, a deep learning model that uses free-text medical notes from the first 48 hours of stay to predict patient mortality in the ICU. We use the MIMIC-III database [10] to train a convolutional neural network (ConvNet) that is able to use raw nursing notes with minimal preprocessing to efficiently generate a prediction, and we couple the prediction of mortality with word importance and sentence importance visualizations, in a way that annotates the original medical note to show what parts of it are more predictive for death or survival, according to the model.", "cite_spans": [{"start": 192, "end": 196, "text": "[10]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Introduction"}, {"text": "In the past some works have used deep learning to predict ICU mortality using free text. Grnarova et al [8] proposed the use of a convolutional neural network for ICU mortality prediction using free-text medical notes from MIMIC-III. They used all medical notes from each patient stay to predict mortality, and trained their model using a custom loss function that included a cross-entropy term involving mortality prediction at the sentence as well, with promising results. Jo et al [11] used a hybrid Latent Dirichlet Allocation (LDA) + Long Short Term Memory (LSTM) model for ICU mortality prediction trained on medical notes from MIMIC-III, in which the LSTM used the topic LDA features as input. Suchil et al [12] used stacked denoising autoencoders to create patient representations out of medical free-text notes, to be used for downstream tasks as mortality prediction. Si et al [13] proposed the use of a ConvNet for multitask prediction (mortality, length of stay), using all available patient medical notes up until time of discharge. Jin et al [14] proposed a multimodal neural network architecture and a Named Entity Recognition (NER) text pre-processing pipeline to predict in-hospital ICU mortality using all available types of free-text notes and a set of vital signs and lab results from the first 48 hours of patient stay, extracted from MIMIC-III.", "cite_spans": [{"start": 104, "end": 107, "text": "[8]", "ref_id": "BIBREF8"}, {"start": 484, "end": 488, "text": "[11]", "ref_id": "BIBREF12"}, {"start": 714, "end": 718, "text": "[12]", "ref_id": "BIBREF13"}, {"start": 887, "end": 891, "text": "[13]", "ref_id": "BIBREF14"}, {"start": 1056, "end": 1060, "text": "[14]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Related work"}, {"text": "Most of these works include some ad-hoc interpretability mechanism:", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "Grnarova et al [8] included a sentence-based mortality prediction target which is then used to score individual words according to their associated predicted mortality probability, Jo et al [11] used LDA-computed weights to provide word importance, Suchil et al [12] used a gradient-based interpretability approach to compute the importance of words in the input notes.", "cite_spans": [{"start": 15, "end": 18, "text": "[8]", "ref_id": "BIBREF8"}, {"start": 190, "end": 194, "text": "[11]", "ref_id": "BIBREF12"}, {"start": 262, "end": 266, "text": "[12]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Related work"}, {"text": "Our work has key differences relative to those from the related literature.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "As opposed to [8, 13] , we only use notes from the first 48 hours of patient stay instead of all notes available up until the time of discharge/death, and as opposed to citejin2018improving we only use nursing notes and not the whole spectrum of notes available in MIMIC-III. Also from an interpretability standpoint we rely on a theoretically sound concept from coalitional game theory, known as the Shapley Value [17] , instead of explainability heuristics.", "cite_spans": [{"start": 14, "end": 17, "text": "[8,", "ref_id": "BIBREF8"}, {"start": 18, "end": 21, "text": "13]", "ref_id": "BIBREF14"}, {"start": 415, "end": 419, "text": "[17]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Related work"}, {"text": "Finally our visualization approach puts emphasis on presenting results in a way that can be easily understood and it is useful for users.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "The contributions of our work are summarized in the following:", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "\u2022 We present a model that is able to offer performance comparable to state of the art models that use physiological time series data, but only using raw nursing notes extracted from MIMIC-III.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "\u2022 Our approach only uses data from the first 48 hours of patient stay, instead of using data from the entirety of the stay. That makes our model more usable in a real setting as a benchmark tool.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "\u2022 Our approach to interpretability is based on a theoretically sound concept (the Shapley Value) and our visualizations provide a novel way to annotate clinical free-text notes to highlight the most informative parts for the prediction of mortality. This paper is organized as follows: first we will show the overall distribution of our patient cohort dataset and its corresponding distribution of medical free-text notes. Then we will briefly describe our approach to interpretability using the Shapley Value, followed by a description of our convolutional architecture and experimental setting. Finally we will present and discuss our results and end with our conclusions and suggested future work.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "We used the Medical Information Mart for Intensive Care (MIMIC-III v1.4) to create a dataset for the training of our deep learning model. MIMIC-III contains ICU records including vitals, laboratory, therapeutical and radiology reports, representing more than a decade of data from patients admitted to the ICUs of the Beth Israel Deaconess Center in Boston, Massachusetts [10] . The median age of adult patients (those with age > 16y) is 65.8 years, and the median length of stay (LoS) for ICU patients is 2.1 days (Q1-Q3:", "cite_spans": [{"start": 372, "end": 376, "text": "[10]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Participants"}, {"text": "Our patient cohort was created using the following criteria: only stays longer than 48 hours were considered, in cases where patients were admitted multiple times to the ICU only the first admission was considered, and patients should have at least one free-text note recorded during their ICU stay.", "cite_spans": [], "ref_spans": [], "section": "Participants"}, {"text": "These criteria lead to a sample with n = 21415. Table 1 shows the different types of medical notes included in our dataset together with their respective counts.", "cite_spans": [], "ref_spans": [{"start": 48, "end": 55, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Participants"}, {"text": "Given that a substantial number of patients in our dataset were missing more than one type of medical note, and that nursing and nursing/other types were the more prevalent ones, we decided to only include patients that had some type of nursing note available (nursing, nursing/other), with no regard to the note word count. This reduced our patient sample to n = 16970, with 1659 recorded deaths (9.78%) and 15311 patients that survived (90.22%). The mean note length is 1252.59 words, with a standard deviation of 1087.48. ", "cite_spans": [], "ref_spans": [], "section": "Participants"}, {"text": "Our prediction model, called ISeeU2, is a convolutional neural network (ConvNet). ConvNets are a specialized neural network architecture that ex- ploit the convolution operator and spatial pooling operations to detect local patterns and reduce input dimensionality to learn a representation that is useful for predictive purposes [15] . ConvNets are extensively and primarily used for computer vision but have found application in Natural Language", "cite_spans": [{"start": 330, "end": 334, "text": "[15]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Deep learning model"}, {"text": "Processing as well, given their ability to deal with patterns occurring at different scales in sequential inputs [16, 8] .", "cite_spans": [{"start": 113, "end": 117, "text": "[16,", "ref_id": "BIBREF17"}, {"start": 118, "end": 120, "text": "8]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Deep learning model"}, {"text": "The specific architecture of our model ( figure 4) includes a text embedding layer to convert a bag of words text representation into 10-dimensional dense word vectors. The output of the embedding layer is then fed to a convolutional layer with 32 channels and a kernel size of 5x10 (stride 1), followed by ReLU activations and a max-pooling layer with a pool size of 1x3 (stride 1). The obtained representation is then fed to a 50x1 dense layer with ReLU activations connected to a one-neuron final layer with sigmoid activation, which computes the mortality probability.", "cite_spans": [], "ref_spans": [{"start": 41, "end": 50, "text": "figure 4)", "ref_id": "FIGREF5"}], "section": "Deep learning model"}, {"text": "One argument that is used routinely against deep learning is its reduced interpretability when compared to other modeling techniques such as logistic regression [9] . In order to overcome that potential limitation we use the ", "cite_spans": [{"start": 161, "end": 164, "text": "[9]", "ref_id": null}], "ref_spans": [], "section": "Deep learning model"}, {"text": "The summation is taken over all possible subsets S \u2286 N that don't in- [20] . DeepLIFT is an algorithm specifically designed to compute feature importance in feed-forward neural networks. DeepLIFT overcomes the issues associated with competing methods such as Layerwise Relevance Propagation [19] , and gradient-based attribution [21, 22] , i.e. saturation, overlooking negative contributions, and gradient discontinuities [19] . DeepLIFT computes feature importance by comparing the network output to a reference output obtained by feeding the network with a designated input. The difference in outputs is back-propagated through the different layers of the network until the input layer is reached and feature importances are fully computed. A more detailed treatment of DeepLIFT in the context of interpreting deep learning models for critical care prognosis can be found in [5] .", "cite_spans": [{"start": 70, "end": 74, "text": "[20]", "ref_id": "BIBREF22"}, {"start": 291, "end": 295, "text": "[19]", "ref_id": "BIBREF21"}, {"start": 329, "end": 333, "text": "[21,", "ref_id": "BIBREF23"}, {"start": 334, "end": 337, "text": "22]", "ref_id": "BIBREF24"}, {"start": 422, "end": 426, "text": "[19]", "ref_id": "BIBREF21"}, {"start": 877, "end": 880, "text": "[5]", "ref_id": "BIBREF4"}], "ref_spans": [], "section": "Deep learning model"}, {"text": "Our ConvNet was built using Tensorflow [23] . Since our dataset is highly unbalanced (negative outcomes represent just 9.78% of training examples), we used a weighted logarithmic loss assigning more importance to the posi-tive class, i.e. patients that died in the ICU. We used 5-fold cross-validation to assess the model performance and place a confidence estimate on it. We did not perform any substantial hyperparameter optimization other than conservatively varying the number of channels of the convolutional layer and the number of neurons of the first fully connected layer of the network. Our choice of optimizer was Adam [24] with default Tensorflow-provided parameters. Our model was trained for three epochs per training fold, and we kept the lowest loss model of each run.", "cite_spans": [{"start": 39, "end": 43, "text": "[23]", "ref_id": "BIBREF25"}, {"start": 630, "end": 634, "text": "[24]", "ref_id": "BIBREF26"}], "ref_spans": [], "section": "Results"}, {"text": "One of our goals is to show a deep learning model that needs little to no input pre-processing in order for it to be as widely applicable as possible.", "cite_spans": [], "ref_spans": [], "section": "Text pre-processing"}, {"text": "Keeping with that we used the NLTK library [25] to remove English stopwords and the Tensorflow.keras default tokenizer to vectorize the text notes, keeping the 100k most frequent words; and no further pre-processing was attempted. The tokenizer was fitted only on the training folds to avoid data leakage. Finally, we set the maximum note length to 500, so notes with a larger word count were truncated at the beginning and those with a smaller word count were padded at the beginning with zeroes.", "cite_spans": [{"start": 43, "end": 47, "text": "[25]", "ref_id": null}], "ref_spans": [], "section": "Text pre-processing"}, {"text": "Using this configuration we obtained a 5-fold cross validation Receiver", "cite_spans": [], "ref_spans": [], "section": "Model performance and comparison with baselines"}, {"text": "Operating Characteristic Area Under the Curve (ROC AUC) of 0.8629 (\u00b10.0058) as seen in figure 5 . Using a 0.5 decision threshold, the model reaches 72% sensitivity at 83% specificity. We also provide some baseline models to compare with our proposed model to better assess its performance. Concretely, we have included results for a traditionally used mortality risk score and a recurrent neural network. ", "cite_spans": [], "ref_spans": [{"start": 87, "end": 95, "text": "figure 5", "ref_id": "FIGREF7"}], "section": "Model performance and comparison with baselines"}, {"text": "As baseline, we used a well-established ICU mortality risk score, SAPS-II [26] . SAPS-II uses data from the first 24 hours of ICU stay to calculate a numerical score, which in turn is converted into a mortality probability. In order to compare our approach with SAPS-II predictions and performance, we trained our convolutional architecture using nursing notes from the first 24 hours only while keeping training parameters the same. We used the SAPS-II implementation provided by the authors of the MIMIC-III code repository [27] . The 24 hour version of our model obtained a 0.8155 (\u00b10.0102) ROC AUC 5-fold cross-validation score, against 0.7448 (\u00b10.0117)", "cite_spans": [{"start": 74, "end": 78, "text": "[26]", "ref_id": "BIBREF28"}, {"start": 526, "end": 530, "text": "[27]", "ref_id": "BIBREF29"}], "ref_spans": [], "section": "SAPS-II."}, {"text": "for the SAPS-II model. Figures 6 and 7 show the corresponding ROC plots for the two models. ", "cite_spans": [], "ref_spans": [{"start": 23, "end": 38, "text": "Figures 6 and 7", "ref_id": "FIGREF8"}], "section": "SAPS-II."}, {"text": "Short Term Memory (LSTM). LSTM is a neural network model designed to handle sequential input data with temporal dependencies [28] , and it has been used extensively in Natural Language Processing tasks. We trained a deep neural network with a bidirectional LSTM layer with 100 units, followed by an extra 100-unit LSTM layer, a 50-unit dense layer ReLU activation, and a final sigmoid layer. As it was the case for our original convolutional model, an embedding layer was used to create 10-dimensional dense vectors to feed the initial layer of the LSTM and the same text preprocessing pipeline was used (save for a now 1000-word maximum note length). Finally dropout with probability 0.5 was applied to control overfitting. With this particular architecture we were able to obtain a 0.7839 (\u00b10.0076) ROC AUC 5-fold cross-validation score (Figure 8 ).", "cite_spans": [{"start": 125, "end": 129, "text": "[28]", "ref_id": "BIBREF30"}], "ref_spans": [{"start": 839, "end": 848, "text": "(Figure 8", "ref_id": "FIGREF10"}], "section": "LSTM. Our second baseline is a recurrent neural network based on the Long"}, {"text": "Using the DeepLIFT implementation provided by [20] which works appropriately with Tensorflow 2 models, we calculated word importances for our model, using the empirical mean of the input embedding vectors as ref-", "cite_spans": [{"start": 46, "end": 50, "text": "[20]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Model interpretability"}, {"text": "erence value. Using these values we designed and built visualizations to show (Figures 9 and 10 ).", "cite_spans": [], "ref_spans": [{"start": 78, "end": 95, "text": "(Figures 9 and 10", "ref_id": "FIGREF1"}], "section": "Model interpretability"}, {"text": "Word clouds are an interesting way to visualize words and their impor- tance at the same time, but they don't capture the context in which words live, potentially leading to erroneous interpretations. For example, the survival word cloud in Figure 9 shows melena as associated with survival, which is not readily understandable. However, when the word cloud is combined with the note heatmap, the reason becomes apparent, given the context of the word (stable present wo melena stool ). We also observe that certain phrases and words are flagged intuitively, e.g. guaic pos heme, and also the fact that for this particular patient occurrences of Plavix/Clopidogrel in the Note length and mortality probability. High capacity machine learning models such as deep neural networks have the ability to leverage subtle correlations and patterns to attain very low training error in learning tasks. As shown in Table 2 and Figure 1 , there is a difference in our sample between mean length of patients who survived and those who had a negative outcome. A", "cite_spans": [], "ref_spans": [{"start": 241, "end": 249, "text": "Figure 9", "ref_id": "FIGREF11"}, {"start": 905, "end": 912, "text": "Table 2", "ref_id": "TABREF2"}, {"start": 917, "end": 925, "text": "Figure 1", "ref_id": "FIGREF1"}], "section": "Model interpretability"}, {"text": "Mann-Whitney U test supplied further evidence, as we were able to reject the Having established that, we decided to investigate if our model was attending somehow to that difference in distributions. For this purpose we inspected the importance score of the padding characters used by our preprocessing pipeline, with most of them being regarded as evidence for survival, which is consistent with our original conjecture that the model considers that shorter notes are correlated with a survival outcome (shorter notes have more padding characters). Figure 11 shows the distribution of approximate Shapley Values for padding characters.", "cite_spans": [], "ref_spans": [{"start": 550, "end": 559, "text": "Figure 11", "ref_id": "FIGREF1"}], "section": "Model interpretability"}, {"text": "Our convolutional model shows interesting performance on the MIMIC-III dataset, with consistent results across validation folds, showing evidence Our results are not directly comparable to those published by Grnarova el al [8] given that we restricted our input window to the first 48 hours of patient stay, instead of using all available notes up until the time of discharge.", "cite_spans": [{"start": 223, "end": 226, "text": "[8]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Discussion"}, {"text": "Results published by Jo et al [11] show their models performing under 0.84 ROC AUC for mortality prediction using MIMIC-III data (48 hour mark), which is well below our results here. On the other hand, the model Vital + EntityEmb reported by [14] uses physiological data and a substantial text preprocessing pipeline that involves a second neural network for Named", "cite_spans": [{"start": 30, "end": 34, "text": "[11]", "ref_id": "BIBREF12"}, {"start": 242, "end": 246, "text": "[14]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Discussion"}, {"text": "Entity Recognition. ", "cite_spans": [], "ref_spans": [], "section": "Discussion"}, {"text": "Limitations of our study include the fact that we do not have access to some pre-admission data, and that we are using a retrospective, single center cohort. Also given the moderate size of our dataset we are only reporting cross-validation results without a proper test set result. An additional limitation is that high-quality nursing notes may not be available for a substantial number of patients in other critical care settings, which could hurt the performance of our model. Finally, the common misspellings and other noise present in the medical notes may affect the quality of the explanations, giving rise to counterintuitive results.", "cite_spans": [], "ref_spans": [], "section": "Limitations and future work"}, {"text": "In future work we intend to investigate the usage of a more robust preprocessing pipeline, and assess whether there is any performance improvement attributable to its usage. Also we intend to evaluate how our approach fares in a situation where limited-quality notes are the only training data available. Finally we plan to explore the joint usage of physiological time series data and free-text medical notes to train a multi-modal deep learning model and compare its performance with our current approach.", "cite_spans": [], "ref_spans": [], "section": "Limitations and future work"}, {"text": "In this paper we have presented ISeeU2, a convolutional neural network for the prediction of mortality using free-text nursing notes from MIMIC-III. We showed that our model is able to offer performance competitive with that of much more complex models with little text pre-processing, while at the same time providing visual explanations of feature importance based on coalitional game theory that allow users to gain insight on the reasons behind predicted outcomes. Our visualizations also provide a way to annotate freetext medical notes with markers to flag parts correlated with predictions of survival and death. We have also shown that nursing notes could be rich enough to capture the concepts needed for mortality prediction at a level of accuracy far higher than what is currently possible with traditional statistical techniques.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Critical Care Utilization for the COVID-19 Outbreak in", "authors": [{"first": "G", "middle": [], "last": "Grasselli", "suffix": ""}, {"first": "A", "middle": [], "last": "Pesenti", "suffix": ""}, {"first": "M", "middle": [], "last": "Cecconi", "suffix": ""}], "year": 2020, "venue": "JAMA", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1001/jama.2020.4031"]}}, "BIBREF1": {"ref_id": "b1", "title": "Fair Allocation of Scarce Medical Resources in the Time of Covid-19", "authors": [{"first": "E", "middle": ["J"], "last": "Emanuel", "suffix": ""}, {"first": "G", "middle": [], "last": "Persad", "suffix": ""}, {"first": "R", "middle": [], "last": "Upshur", "suffix": ""}, {"first": "B", "middle": [], "last": "Thome", "suffix": ""}, {"first": "M", "middle": [], "last": "Parker", "suffix": ""}, {"first": "A", "middle": [], "last": "Glickman", "suffix": ""}, {"first": "C", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "C", "middle": [], "last": "Boyle", "suffix": ""}, {"first": "M", "middle": [], "last": "Smith", "suffix": ""}, {"first": "J", "middle": ["P"], "last": "Phillips", "suffix": ""}], "year": 2020, "venue": "New England Journal of Medicine", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1056/nejmsb2005114"]}}, "BIBREF2": {"ref_id": "b2", "title": "Scoring systems in the intensive care unit: A compendium", "authors": [{"first": "A", "middle": ["G"], "last": "Rapsang", "suffix": ""}, {"first": "D", "middle": ["C"], "last": "Shyam", "suffix": ""}], "year": 2014, "venue": "Official Publication of Indian Society of Critical Care Medicine", "volume": "18", "issn": "4", "pages": "220--228", "other_ids": {"DOI": ["10.4103/0972-5229.130573"]}}, "BIBREF3": {"ref_id": "b3", "title": "Benchmarking Deep Learning Models on Large Healthcare Datasets", "authors": [{"first": "S", "middle": [], "last": "Purushotham", "suffix": ""}, {"first": "C", "middle": [], "last": "Meng", "suffix": ""}, {"first": "Z", "middle": [], "last": "Che", "suffix": ""}, {"first": "Y", "middle": [], "last": "Liu", "suffix": ""}], "year": 2018, "venue": "Journal of Biomedical Informatics", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.jbi.2018.04.007"]}}, "BIBREF4": {"ref_id": "b4", "title": "ISeeU: Visually interpretable deep learning for mortality prediction inside the ICU", "authors": [{"first": "W", "middle": [], "last": "Caicedo-Torres", "suffix": ""}, {"first": "J", "middle": [], "last": "Gutierrez", "suffix": ""}], "year": 2019, "venue": "Journal of Biomedical Informatics", "volume": "98", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/J.JBI.2019.103269"]}}, "BIBREF5": {"ref_id": "b5", "title": "Deep Learning in Medical Image Analysis", "authors": [{"first": "D", "middle": [], "last": "Shen", "suffix": ""}, {"first": "G", "middle": [], "last": "Wu", "suffix": ""}, {"first": "H.-I", "middle": [], "last": "Suk", "suffix": ""}], "year": 2017, "venue": "", "volume": "19", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1146/annurev-bioeng-071516-044442"]}}, "BIBREF7": {"ref_id": "b7", "title": "Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis", "authors": [{"first": "B", "middle": [], "last": "Shickel", "suffix": ""}, {"first": "P", "middle": ["J"], "last": "Tighe", "suffix": ""}, {"first": "A", "middle": [], "last": "Bihorac", "suffix": ""}, {"first": "P", "middle": [], "last": "Rashidi", "suffix": ""}], "year": 2018, "venue": "IEEE journal of biomedical and health informatics", "volume": "22", "issn": "5", "pages": "1589--1604", "other_ids": {"DOI": ["10.1109/JBHI.2017.2767063"]}}, "BIBREF8": {"ref_id": "b8", "title": "Neural Document Embeddings for Intensive Care Patient Mortality Prediction", "authors": [{"first": "P", "middle": [], "last": "Grnarova", "suffix": ""}, {"first": "F", "middle": [], "last": "Schmidt", "suffix": ""}, {"first": "S", "middle": ["L"], "last": "Hyland", "suffix": ""}, {"first": "C", "middle": [], "last": "Eickhoff", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "An evaluation of machine-learning methods for predicting pneumonia mortality", "authors": [{"first": "C", "middle": [], "last": "Janosky", "suffix": ""}, {"first": "T", "middle": [], "last": "Meek", "suffix": ""}, {"first": "T", "middle": [], "last": "Mitchell", "suffix": ""}, {"first": "P", "middle": [], "last": "Richardson", "suffix": ""}, {"first": "", "middle": [], "last": "Spirtes", "suffix": ""}], "year": 1997, "venue": "Artificial Intelligence in Medicine", "volume": "", "issn": "", "pages": "367--370", "other_ids": {"DOI": ["10.1016/S0933-3657(96)00367-3"]}}, "BIBREF11": {"ref_id": "b11", "title": "MIMIC-III, a freely accessible critical care database", "authors": [{"first": "A", "middle": ["E W"], "last": "Johnson", "suffix": ""}, {"first": "T", "middle": ["J"], "last": "Pollard", "suffix": ""}, {"first": "L", "middle": [], "last": "Shen", "suffix": ""}, {"first": "L.-W", "middle": ["H"], "last": "Lehman", "suffix": ""}, {"first": "M", "middle": [], "last": "Feng", "suffix": ""}, {"first": "M", "middle": [], "last": "Ghassemi", "suffix": ""}, {"first": "B", "middle": [], "last": "Moody", "suffix": ""}, {"first": "P", "middle": [], "last": "Szolovits", "suffix": ""}, {"first": "L", "middle": ["A"], "last": "Celi", "suffix": ""}, {"first": "R", "middle": ["G"], "last": "Mark", "suffix": ""}], "year": 2016, "venue": "Sci Data", "volume": "3", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/sdata.2016.35"]}}, "BIBREF12": {"ref_id": "b12", "title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction", "authors": [{"first": "Y", "middle": [], "last": "Jo", "suffix": ""}, {"first": "L", "middle": [], "last": "Lee", "suffix": ""}, {"first": "S", "middle": [], "last": "Palaskar", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Patient representation learning and interpretable evaluation using clinical notes", "authors": [{"first": "M", "middle": [], "last": "Sushil", "suffix": ""}, {"first": "S", "middle": [], "last": "\u0160uster", "suffix": ""}, {"first": "K", "middle": [], "last": "Luyckx", "suffix": ""}, {"first": "W", "middle": [], "last": "Daelemans", "suffix": ""}], "year": 2018, "venue": "Journal of Biomedical Informatics", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.jbi.2018.06.016"], "arXiv": ["arXiv:1807.01395"]}}, "BIBREF14": {"ref_id": "b14", "title": "Deep Patient Representation of Clinical Notes via Multi-Task Learning for Mortality Prediction", "authors": [{"first": "Y", "middle": [], "last": "Si", "suffix": ""}, {"first": "K", "middle": [], "last": "Roberts", "suffix": ""}], "year": 2019, "venue": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Kass-hout, Improving Hospital Mortality Prediction with", "authors": [{"first": "M", "middle": [], "last": "Jin", "suffix": ""}, {"first": "M", "middle": ["T"], "last": "Bahadori", "suffix": ""}, {"first": "A", "middle": [], "last": "Colak", "suffix": ""}, {"first": "P", "middle": [], "last": "Bhatia", "suffix": ""}, {"first": "B", "middle": [], "last": "Celikkaya", "suffix": ""}, {"first": "R", "middle": [], "last": "Bhakta", "suffix": ""}, {"first": "S", "middle": [], "last": "Senthivel", "suffix": ""}, {"first": "M", "middle": [], "last": "Khalilia", "suffix": ""}, {"first": "D", "middle": [], "last": "Navarro", "suffix": ""}, {"first": "B", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "T", "middle": [], "last": "Doman", "suffix": ""}, {"first": "A", "middle": [], "last": "Ravi", "suffix": ""}, {"first": "M", "middle": [], "last": "Liger", "suffix": ""}, {"first": "T", "middle": [], "last": "", "suffix": ""}], "year": 2018, "venue": "Medical Named Entities and Multimodal Learning", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1811.12276"]}}, "BIBREF16": {"ref_id": "b16", "title": "Gradient-Based Learning Applied to Document Recognition", "authors": [{"first": "Y", "middle": [], "last": "Lecun", "suffix": ""}, {"first": "L", "middle": [], "last": "Bottou", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}, {"first": "", "middle": [], "last": "Haffner", "suffix": ""}], "year": 1998, "venue": "Proceedings of the IEEE", "volume": "86", "issn": "", "pages": "2278--2324", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Deep Learning", "authors": [{"first": "I", "middle": [], "last": "Goodfellow", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}, {"first": "A", "middle": [], "last": "Courville", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "A Value for n-Person Games", "authors": [{"first": "L", "middle": ["S"], "last": "Shapley", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Contributions to the Theory of Games II", "authors": [{"first": "", "middle": [], "last": "Tucker", "suffix": ""}], "year": 1953, "venue": "", "volume": "", "issn": "", "pages": "307--317", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "An Efficient Explanation of Individual Classifications using Game Theory", "authors": [{"first": "E", "middle": [], "last": "Strumbelj", "suffix": ""}, {"first": "I", "middle": [], "last": "Kononenko", "suffix": ""}, {"first": "S", "middle": [], "last": "Wrobel", "suffix": ""}], "year": 2010, "venue": "Journal of Machine Learning Research", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1145/2858036.2858529"], "arXiv": ["arXiv:1606.05386"]}}, "BIBREF21": {"ref_id": "b21", "title": "Learning Important Features Through Propagating Activation Differences", "authors": [{"first": "A", "middle": [], "last": "Shrikumar", "suffix": ""}, {"first": "P", "middle": [], "last": "Greenside", "suffix": ""}, {"first": "A", "middle": [], "last": "Kundaje", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1704.02685"]}}, "BIBREF22": {"ref_id": "b22", "title": "A unified approach to interpreting model predictions", "authors": [{"first": "S", "middle": ["M"], "last": "Lundberg", "suffix": ""}, {"first": "S", "middle": ["I"], "last": "Lee", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1705.07874"]}}, "BIBREF23": {"ref_id": "b23", "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "authors": [{"first": "K", "middle": [], "last": "Simonyan", "suffix": ""}, {"first": "A", "middle": [], "last": "Vedaldi", "suffix": ""}, {"first": "A", "middle": [], "last": "Zisserman", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Striving for Simplicity: The All Convolutional Net", "authors": [{"first": "J", "middle": ["T"], "last": "Springenberg", "suffix": ""}, {"first": "A", "middle": [], "last": "Dosovitskiy", "suffix": ""}, {"first": "T", "middle": [], "last": "Brox", "suffix": ""}, {"first": "M", "middle": ["A"], "last": "Riedmiller", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6806"]}}, "BIBREF25": {"ref_id": "b25", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "authors": [{"first": "M", "middle": [], "last": "Abadi", "suffix": ""}, {"first": "A", "middle": [], "last": "Agarwal", "suffix": ""}, {"first": "P", "middle": [], "last": "Barham", "suffix": ""}, {"first": "E", "middle": [], "last": "Brevdo", "suffix": ""}, {"first": "Z", "middle": [], "last": "Chen", "suffix": ""}, {"first": "C", "middle": [], "last": "Citro", "suffix": ""}, {"first": "G", "middle": [], "last": "Corrado", "suffix": ""}, {"first": "A", "middle": [], "last": "Davis", "suffix": ""}, {"first": "J", "middle": [], "last": "Dean", "suffix": ""}, {"first": "M", "middle": [], "last": "Devin", "suffix": ""}, {"first": "S", "middle": [], "last": "Ghemawat", "suffix": ""}, {"first": "I", "middle": [], "last": "Goodfellow", "suffix": ""}, {"first": "A", "middle": [], "last": "Harp", "suffix": ""}, {"first": "G", "middle": [], "last": "Irving", "suffix": ""}, {"first": "M", "middle": [], "last": "Isard", "suffix": ""}, {"first": "Y", "middle": [], "last": "Jia", "suffix": ""}, {"first": "L", "middle": [], "last": "Kaiser", "suffix": ""}, {"first": "M", "middle": [], "last": "Kudlur", "suffix": ""}, {"first": "J", "middle": [], "last": "Levenberg", "suffix": ""}, {"first": "D", "middle": [], "last": "Man", "suffix": ""}, {"first": "R", "middle": [], "last": "Monga", "suffix": ""}, {"first": "S", "middle": [], "last": "Moore", "suffix": ""}, {"first": "D", "middle": [], "last": "Murray", "suffix": ""}, {"first": "J", "middle": [], "last": "Shlens", "suffix": ""}, {"first": "B", "middle": [], "last": "Steiner", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "P", "middle": [], "last": "Tucker", "suffix": ""}, {"first": "V", "middle": [], "last": "Vanhoucke", "suffix": ""}, {"first": "V", "middle": [], "last": "Vasudevan", "suffix": ""}, {"first": "O", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "P", "middle": [], "last": "Warden", "suffix": ""}, {"first": "M", "middle": [], "last": "Wicke", "suffix": ""}, {"first": "Y", "middle": [], "last": "Yu", "suffix": ""}, {"first": "X", "middle": [], "last": "Zheng", "suffix": ""}], "year": 2015, "venue": "", "volume": "1", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/nn.3331"], "arXiv": ["arXiv:1603.04467"]}}, "BIBREF26": {"ref_id": "b26", "title": "Adam: A Method for Stochastic Optimization", "authors": [{"first": "D", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "J", "middle": [], "last": "Ba", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}}, "BIBREF28": {"ref_id": "b28", "title": "A New Simplified Acute Physiology Score (SAPS II", "authors": [{"first": "J", "middle": ["R"], "last": "Gall", "suffix": ""}, {"first": "S", "middle": [], "last": "Lemeshow", "suffix": ""}, {"first": "F", "middle": [], "last": "Saulnier", "suffix": ""}], "year": 1993, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1001/jama.1993.03510240069035"]}}, "BIBREF29": {"ref_id": "b29", "title": "The MIMIC Code Repository: Enabling reproducibility in critical care research", "authors": [{"first": "A", "middle": ["E"], "last": "Johnson", "suffix": ""}, {"first": "D", "middle": ["J"], "last": "Stone", "suffix": ""}, {"first": "L", "middle": ["A"], "last": "Celi", "suffix": ""}, {"first": "T", "middle": ["J"], "last": "Pollard", "suffix": ""}], "year": 2018, "venue": "Journal of the American Medical Informatics Association", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1093/jamia/ocx084"]}}, "BIBREF30": {"ref_id": "b30", "title": "Long Short-Term Memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {"DOI": ["10.1162/neco.1997.9.8.1735"]}}, "BIBREF31": {"ref_id": "b31", "title": "The Mythos of Model Interpretability, ICML Workshop on Human Interpretability in Machine Learning", "authors": [{"first": "Z", "middle": ["C"], "last": "Lipton", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "96--100", "other_ids": {"arXiv": ["arXiv:arXiv:1606.03490v1"]}}, "BIBREF32": {"ref_id": "b32", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "authors": [{"first": "Z", "middle": [], "last": "Che", "suffix": ""}, {"first": "S", "middle": [], "last": "Purushotham", "suffix": ""}, {"first": "K", "middle": [], "last": "Cho", "suffix": ""}, {"first": "D", "middle": [], "last": "Sontag", "suffix": ""}, {"first": "Y", "middle": [], "last": "Liu", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF33": {"ref_id": "b33", "title": "Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series", "authors": [{"first": "Z", "middle": ["C"], "last": "Lipton", "suffix": ""}, {"first": "D", "middle": [], "last": "Kale", "suffix": ""}, {"first": "R", "middle": [], "last": "Wetzel", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 1st Machine Learning for Healthcare Conference", "volume": "56", "issn": "", "pages": "253--270", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "Temporal convolutional neural networks for diagnosis lab tests", "authors": [{"first": "N", "middle": [], "last": "Razavian", "suffix": ""}, {"first": "D", "middle": [], "last": "Sontag", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "1--17", "other_ids": {"DOI": ["10.1051/0004-6361/201527329"], "arXiv": ["arXiv:1151.07938v1"]}}}, "ref_entries": {"FIGREF0": {"text": "hand deep learning holds promise to positively impact clinical practice by leveraging medical data to assist diagnosis and prediction, including mortality prediction. However, as the question of whether powerful Deep Learning models attend correlations backed by sound medical knowledge when generating predictions remains open, additional interpretability tools are needed to foster trust and encourage the use of AI by clinicians. In this work we show a Deep Learning model trained on MIMIC-III to predict mortality using raw nursing notes, together with visual explanations for word importance. Our model reaches a ROC of 0.8629 (\u00b10.0058), outperforming the traditional SAPS-II score and providing enhanced interpretability when compared with similar Deep Learning approaches.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Estimated nursing notes length distribution.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Histogram of age distribution by outcome. As a result of privacy preserving measures, MIMIC-III shifts ages greater than 89 years (i.e. patients appear to be 300 years old).", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Histogram of length of stay distribution by outcome.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Value in order to find how inputs affect the output of the model, hence gaining insight about which kinds of note fragments are more correlated with negative outcomes. The Shapley Value is a concept from coalitional game theory that formalizes the contribution of individual players towards the attainment of a goal as part of a team. The Shapley value captures", "latex": null, "type": "figure"}, "FIGREF5": {"text": "Deep learning model architecture.the marginal importance of each player when its role is analyzed across all possible subsets of players from the original coalition. According to Shapley[17], given a coalitional form game N, v , with a finite set of players N of size n and a function v : 2 N \u2192 R that describes the total worth of the coalition, the marginal importance of player i can be expressed as", "latex": null, "type": "figure"}, "FIGREF6": {"text": "clude player i, and each of its terms captures the effect of player i on the reward attained by each subset, v(S \u222a {i}) \u2212 v(S).", "latex": null, "type": "figure"}, "FIGREF7": {"text": "ConvNet 5-fold cross validated ROC AUC.", "latex": null, "type": "figure"}, "FIGREF8": {"text": "SAPS-II model 5-fold cross validated ROC AUC.", "latex": null, "type": "figure"}, "FIGREF9": {"text": "ConvNet (24h) 5-fold cross validated ROC AUC.", "latex": null, "type": "figure"}, "FIGREF10": {"text": "Deep LSTM 5-fold cross validated ROC AUC. the importance of each word in the original nursing note used as input. Our visualizations constitute a form of post hoc interpretability [29] insofar as they try to convey how the model regards the inputs in terms of their impact on the predicted probability of death, without having to explain the internal mechanisms of our neural network, nor sacrificing predictive performance. We have selected some examples at random from both the training set and the validation set of the last cross-validation run to show the behavior of the model and the way it regards certain words in the input notes. Our proposed visualizations include word clouds and text heatmaps", "latex": null, "type": "figure"}, "FIGREF11": {"text": "Top: Word clouds generated for one specific patient in the training set show the words deemed as most important for both survival (left) and death (right) prediction. Bottom: Text heatmap showing words, their importance and their context in sentences, generated for one specific patient in the training set. Red color denotes evidence for death, and blue color represents evidence for survival. Words with a gray background are not considered important for the prediction task by the network. Padding characters are represented by asterisks.", "latex": null, "type": "figure"}, "FIGREF12": {"text": "note are flagged as evidence for survival. There are other instances in which results are not intuitive and may point to statistical flukes rather than strong causal features. As an example we can point to the phrases return baseline bp numerous large clots suctioned, and continue keep pt family aware, in which the words clot and pt seem to be flagged incorrectly.Annotation smoothing. In order to help ameliorate the sharp changes and inconsistencies observed at the sentence level we used a convolution filter to take into account the effect of the Shapley Values of all words in a particular sentence when generating the heatmap annotations, and provide a smoother and more intuitive result. Note that this is approximated since we are intent on using a basic pre-processing pipeline,, without any advanced capabilities (i.e. sentence segmentation). A 5 \u00d7 1 convolution filter [to spread out the feature importance of individual words and to fade out weak importances that are due possibly to noise, while still keeping the most salient features.Figure 10show our previous training set and a new validation set note with and without the convolution filter applied.", "latex": null, "type": "figure"}, "FIGREF13": {"text": "Text heatmaps with (right) and without (left) convolution smoothing. Bottom row corresponds to a nursing note from the validation set. null hypothesis, i.e. distributions of the length of nursery notes are the same (p = 0.000), in favor of our alternative hypothesis, i.e. notes for patients that do not survive are longer.", "latex": null, "type": "figure"}, "FIGREF14": {"text": "Distribution of approximate Shapley Values for padding characters. The histogram shows that most padding characters are deemed as evidence for survival by our model. for good generalization. Validation ROC AUC (95% CI [0.855689, 0.867888])is competitive with published results in a comprehensive benchmark by[4] (95% CI [0.873706, 0.882894] ROC AUC) and our previous work[5] (95% CI[0.870396, 0.876604] ROC AUC). Moreover, our model bypass some of the most important difficulties associated with the usage of physiological time series, i.e. inconsistent sampling times and missing values. On the other hand, the 24-hour version of our model still manages to surpass comfortably the SAPS-II baseline.", "latex": null, "type": "figure"}, "TABREF1": {"text": "Distribution of free-text medical notes in our dataset.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Note Type </td><td>Count </td><td>Percentage\n</td></tr><tr><td>Nursing/other </td><td>83147 </td><td>36.78%\n</td></tr><tr><td>Radiology </td><td>61096 </td><td>27.02%\n</td></tr><tr><td>Nursing </td><td>43790 </td><td>19.37%\n</td></tr><tr><td>Physician </td><td>27789 </td><td>12.30%\n</td></tr><tr><td>Respiratory </td><td>5728 </td><td>2.53%\n</td></tr><tr><td>General </td><td>1775 </td><td>0.78%\n</td></tr><tr><td>Nutrition </td><td>1549 </td><td>0.68%\n</td></tr><tr><td>Rehab Services </td><td>521 </td><td>0.23%\n</td></tr><tr><td>Social Work </td><td>501 </td><td>0.22%\n</td></tr><tr><td>Case Management </td><td>134 </td><td>0.060%\n</td></tr><tr><td>Consult </td><td>40 </td><td>0.018%\n</td></tr><tr><td>Pharmacy </td><td>12 </td><td>0.0053%\n</td></tr><tr><td>Overall </td><td>226082 </td><td>100%\n</td></tr></table></body></html>"}, "TABREF2": {"text": "and figure 1 show details about the distribution of notes' lengths. The median age of patients in our final sample is 67.2 years, and the median length of stay is 3.96 days (Q1-Q3:2.8-7.16). Figures 2 and 3 show the distribution of age and length of stay in our final sample.", "latex": null, "type": "table"}, "TABREF3": {"text": "Length distribution of nursing notes for our patient sample.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Statistic </td><td>Positive class (survival) </td><td>Negative class (death) </td><td>Overall\n</td></tr><tr><td>Count </td><td>15311 </td><td>1659 </td><td>16970\n</td></tr><tr><td>Mean </td><td>1233.3 </td><td>1430.6 </td><td>1252.6\n</td></tr><tr><td>Std </td><td>1083 </td><td>1112.4 </td><td>1087.5\n</td></tr><tr><td>Min </td><td>34 </td><td>144 </td><td>34\n</td></tr><tr><td>Q1 </td><td>711.0 </td><td>890 </td><td>724\n</td></tr><tr><td>Q2 </td><td>934 </td><td>1135 </td><td>952\n</td></tr><tr><td>Q3 </td><td>1286 </td><td>1492 </td><td>1310\n</td></tr><tr><td>Max </td><td>33771 </td><td>9756 </td><td>33771\n</td></tr></table></body></html>"}, "TABREF4": {"text": "Strumbelj et al[18] have shown in their work that it is possible to apply the Shapley Value to the problem of feature importance quantification, if inputs are considered players in a coalition, and the predicted value is akin to the attained reward. In this way the Shapley Value becomes very use-ful, as it takes into account the interaction between features, in a way other methods like tree-based feature importance or simple input occlusion cannot.", "latex": null, "type": "table"}, "TABREF5": {"text": "shows reported performance results for relevant models, compared with the performance of our model.", "latex": null, "type": "table"}, "TABREF6": {"text": "Our results and results reported by related works. ROC AUC results are mean and standard deviation from a 5-fold cross validation run, except LSTM+E+T+D and Vital + EntityEmb, which report a single result over the test set. using text notes as input we are using not the raw physiological data but healthcare workers perceptions and judgement in the form of free-text notes, giving us access to higher level concepts not present in said physiological data. On the other hand, MIMIC-III notes are very noisy, with frequent misspellings, typos and a lack of standardized naming (i.e. writing vancomycin vs vanc), which makes them not an optimal learning substrate. However, we have been able to show that deep learning models are able to separate useful signals from such noise, by keeping our pre-processing pipeline very basic. Another interesting take on the usage of free text notes is that deep models can leverage meta-data such as note length, as our evidence suggest. This phenomena is comparable to observations made in[31,32] regarding the ability of deep neural networks to exploit patterns of missingness in physiological patient data to attain better predictive performance: Certain physiological measurements are taken more or less frequently according to the state of the patient, providing additional and useful metadata. This kind of flexibility and power is outside the reach of more traditional statistical modeling techniques as the ones behind risk scores as SAPS-II.Our visualization approach allows to easily locate the parts of the notes the deep learning model is attending to, which can then be compared to clinical expectations. In this way the potential users can be certain that the reasons behind the predictions are sound and align properly with medical knowledge, as opposed to being evidence of statistical artifacts being leveraged by the model. It is interesting to note that our results suggest our model and text heatmap visualization could be used to annotate medical notes for, at some point, easier handling by ICU staff. Finally, it's worth noticing that our usage of Shapley Values was instrumental to discover how the network regarded the padding introduced in shorter nursing notes.", "latex": null, "type": "table"}}, "back_matter": []}